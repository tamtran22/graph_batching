{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data.dataset import DatasetLoader, OneDDatasetLoader, OneDDatasetBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing.batching import _get_graph_partition\n",
    "from networks.loss import *\n",
    "from preprocessing.normalize import normalize_graph\n",
    "from networks.network_parc import PARC\n",
    "from data.file_reader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading raw dataset\n",
    "# dataset_raw = OneDDatasetLoader(\n",
    "#     root_dir='/data1/tam/downloaded_datasets'\n",
    "# )\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_transformed'\n",
    ")\n",
    "# print(dataset_raw[0].edge_attr[1000])\n",
    "# print(dataset_transformed[0].edge_attr[1000])\n",
    "# dataset.normalizing(sub_dir='/normalized/')\n",
    "dataset[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start normalizing..\n",
      "Finish normalizing.\n"
     ]
    }
   ],
   "source": [
    "# process dataset all\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_in',\n",
    "    data_names='all'\n",
    ")\n",
    "print('Start normalizing..')\n",
    "_dataset = dataset.normalizing(sub_dir='/normalized')\n",
    "print('Finish normalizing.')\n",
    "# print('Start batching..')\n",
    "# __dataset = _dataset.batching(batch_size=500, batch_n_times=40, recursive=True,\n",
    "#                             sub_dir='/batched/')\n",
    "# print('Finish batching.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([59769, 5, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_in',\n",
    "    sub_dir='/normalized'\n",
    ")\n",
    "data = dataset[0]\n",
    "data\n",
    "model = PARC(2, 5, 128, 10)\n",
    "\n",
    "edge_index = data.edge_index\n",
    "\n",
    "mesh = edge_to_node(data.node_attr.numpy(), edge_index.numpy())\n",
    "mesh = torch.tensor(mesh).float()\n",
    "\n",
    "pres = data.pressure[:,0].unsqueeze(1)\n",
    "vel = data.velocity[:,0].unsqueeze(1)\n",
    "# vel = edge_to_node(vel.numpy(), edge_index.numpy())\n",
    "# vel = torch.tensor(vel)\n",
    "F_init = torch.cat([pres, vel], dim=-1).float()\n",
    "\n",
    "out = model.forward(F_init, mesh, edge_index)\n",
    "out[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([59769, 5, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fs_hat = torch.cat([data.pressure[:,0:5].unsqueeze(-1), data.velocity[:,0:5].unsqueeze(-1)], dim=-1)\n",
    "Fs_hat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_transformed[0]\n",
    "# print(data)\n",
    "graph_part = _get_graph_partition(data=data, partition=np.arange(5,15), recursive=True)\n",
    "# print(graph_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching dataset\n",
    "# batched_dataset = dataset_transformed.batching(batch_size=5000, batch_n_times=10, recursive=True,\n",
    "#                                     sub_dir='/batched/')\n",
    "_dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_transformed',\n",
    "    sub_dir='/batched/'\n",
    ")\n",
    "_dataset[0].edge_attr[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming/scaling dataset\n",
    "dataset = dataset\n",
    "p_min = dataset.min(var_name='pressure', axis=None).numpy()\n",
    "p_max = dataset.max(var_name='pressure', axis=None).numpy()\n",
    "print(p_min, p_max)\n",
    "q_min = dataset.min(var_name='flowrate', axis=None).numpy()\n",
    "q_max = dataset.max(var_name='flowrate', axis=None).numpy()\n",
    "print(q_min, q_max)\n",
    "u_min = dataset.min(var_name='velocity', axis=None).numpy()\n",
    "u_max = dataset.max(var_name='velocity', axis=None).numpy()\n",
    "print(u_min, u_max)\n",
    "\n",
    "ea_min = dataset.min(var_name='edge_attr', axis=0).numpy()\n",
    "ea_max = dataset.max(var_name='edge_attr', axis=0).numpy()\n",
    "vol0_min = ea_min[-2]\n",
    "vol1_min = ea_min[-1]\n",
    "vol0_max = ea_max[-2]\n",
    "vol1_max = ea_max[-1]\n",
    "ea_min[-2] = min(vol0_min, vol1_min)\n",
    "ea_min[-1] = min(vol0_min, vol1_min)\n",
    "ea_max[-2] = max(vol0_max, vol1_max)\n",
    "ea_max[-1] = max(vol0_max, vol1_max)\n",
    "print(ea_min, ea_max)\n",
    "\n",
    "\n",
    "# p_mean = dataset.mean(var_name='pressure', axis=None).numpy()\n",
    "# p_std = dataset.std(var_name='pressure', axis=None).numpy()\n",
    "# print(p_mean, p_std)\n",
    "# q_mean = dataset.mean(var_name='flowrate', axis=None).numpy()\n",
    "# q_std = dataset.std(var_name='flowrate', axis=None).numpy()\n",
    "# print(q_mean, q_std)\n",
    "# u_mean = dataset.mean(var_name='velocity', axis=None).numpy()\n",
    "# u_std = dataset.std(var_name='velocity', axis=None).numpy()\n",
    "# print(u_mean, u_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting pressure distribution\n",
    "\n",
    "p = dataset[0].pressure.numpy()[100,:]\n",
    "\n",
    "# p = -1+2*(p-p_min)/(p_max-p_min)\n",
    "# p = (p-p_mean)/(p_std+1e-10)\n",
    "# plt.ylim(-1,1)\n",
    "# plt.hist(p, bins=1000)\n",
    "plt.plot(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting flowrate distribution\n",
    "\n",
    "q = dataset[0].flowrate.numpy()[50000,:]\n",
    "\n",
    "# q = (q-q_mean)/(q_std+1e-10)\n",
    "# q = -1+2*(q-q_min)/(q_max-q_min)\n",
    "# plt.xlim(-1,1)\n",
    "# plt.hist(q, bins=10)\n",
    "plt.plot(list(range(len(q))), q)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume\n",
    "vol0_raw = dataset_raw[0].edge_attr[:,-2].numpy()\n",
    "vol0_transformed = dataset_transformed[0].edge_attr[:,-1].numpy()\n",
    "print(vol0_raw.shape, vol0_transformed.shape)\n",
    "print(vol0_raw[1000], vol0_transformed[1000])\n",
    "\n",
    "print(vol0_raw.mean(), vol0_raw.std())\n",
    "print(vol0_transformed.mean(), vol0_transformed.std())\n",
    "\n",
    "# v_min = min(vol0.min(), vol1.min())\n",
    "# v_max = max(vol0.max(), vol1.max())\n",
    "\n",
    "# vol0 = -1+2*(vol0 - v_min)/(v_max - v_min)\n",
    "# vol1 = -1+2*(vol1 - v_min)/(v_max - v_min)\n",
    "\n",
    "# vol0 = (vol0 - vol0.mean())/(vol0.std() + 1e-10)\n",
    "# vol1 = (vol1 - vol1.mean())/(vol1.std() + 1e-10)\n",
    "\n",
    "# vol0 = (vol0 - vol0.min())/(vol0.max() + vol0.min())\n",
    "# vol1 = (vol1 - vol1.min())/(vol1.max() + vol0.min())\n",
    "\n",
    "\n",
    "# vol0 = (vol0 - np.median(vol0))/(np.percentile(vol0, 75) - np.percentile(vol0, 25))\n",
    "# vol1 = (vol1 - np.median(vol1))/(np.percentile(vol1, 75) - np.percentile(vol1, 25))\n",
    "\n",
    "# plt.plot(list(range(vol0.shape[0])), vol0, c='red')\n",
    "# plt.plot(list(range(vol0.shape[0])), vol1, c='blue')\n",
    "# plt.xlim(4350, 4450)\n",
    "# plt.ylim(-0.98,-0.88)\n",
    "\n",
    "plt.plot(vol0_raw)\n",
    "# plt.plot(vol0_transformed)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation scatter plot\n",
    "data = dataset[0]\n",
    "x = data.edge_attr.numpy()[:,6]\n",
    "# y = data.velocity.numpy()[:,50]\n",
    "y = data.flowrate.numpy().mean(axis=1)\n",
    "\n",
    "y = (y - u_min)/(u_max - u_min)\n",
    "# y = np.sign(y)*np.log(1. + np.abs(y)/1e2)\n",
    "# d = data.edge_attr.numpy()[:,1]\n",
    "# y = y / (np.square(d))\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test normalize\n",
    "# data = dataset[0]\n",
    "# print(data)\n",
    "# data = normalize_graph(\n",
    "#     data=data,\n",
    "#     # x_min=0, x_max=512,\n",
    "#     edge_attr_min=ea_min, edge_attr_max=ea_max,\n",
    "#     pressure_min=p_min, pressure_max=p_max,\n",
    "#     velocity_min=u_min, velocity_max=u_max\n",
    "# )\n",
    "# print(data)\n",
    "# plt.plot(data.flowrate_bc[1000,:])\n",
    "# plt.show()\n",
    "\n",
    "# y_true = data.velocity[:,8]\n",
    "# y_pred = data.velocity[:,9]\n",
    "# print(y_true, y_pred, data.weight)\n",
    "# plt.plot(y_true)\n",
    "# plt.plot(y_pred)\n",
    "# plt.show()\n",
    "\n",
    "# loss = weighted_mean_squared_error(y_pred, y_true, torch.tensor(data.weight))\n",
    "# print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weighted loss\n",
    "data = normed_data\n",
    "diam = data.edge_attr.numpy()[:,0]\n",
    "length = data.edge_attr.numpy()[:,1]\n",
    "\n",
    "(count, bins) = np.histogram(diam, bins=5000)\n",
    "n_edge = diam.shape[0]\n",
    "\n",
    "# print(count)\n",
    "# print(bins)\n",
    "\n",
    "def weight(val : float):\n",
    "    bin_id = np.where(bins >= val)[0][0] - 1\n",
    "    weight_mean = 1. / (count[bin_id] + 1e0)\n",
    "    return weight_mean\n",
    "\n",
    "v_weight = np.vectorize(weight)\n",
    "weight_diam = v_weight(diam)\n",
    "\n",
    "\n",
    "def cal_weight(x : np.array, bins=1000) -> np.array:\n",
    "    (count, bin) = np.histogram(x, bins=bins)\n",
    "    N = x.shape[0]\n",
    "\n",
    "    def _weight(value : float):\n",
    "        _bin_id = np.where(bin >= value)[0][0] - 1\n",
    "        _weight = 1. / (count[_bin_id] + 1.)\n",
    "        return _weight\n",
    "    \n",
    "    v_weight = np.vectorize(_weight)\n",
    "\n",
    "    return v_weight(x)\n",
    "\n",
    "plt.plot(bins[:-1], count)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(weight_diam)\n",
    "# plt.xlim(21000,21025)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batching BFS\n",
    "import random\n",
    "data = dataset[0]\n",
    "def BFS_partition(edge_index, partition_size=None, n_partitions=None):\n",
    "    def BFS(edge_index, root, visited, part_size=100):\n",
    "        queue = [root]\n",
    "        visited = []\n",
    "        part = []\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            while current in visited:\n",
    "                current = queue.pop(0)\n",
    "            visited.append(current)\n",
    "            part.append(current)\n",
    "            # find child nodes\n",
    "            child_edges = np.where(edge_index[0] == current)[0]\n",
    "            child_nodes = list(edge_index[1][child_edges])\n",
    "            # add child nodes to queue\n",
    "            queue += child_nodes\n",
    "            # break\n",
    "            if len(part) >= part_size:\n",
    "                break\n",
    "        return part, queue, visited\n",
    "    \n",
    "    if partition_size is None:\n",
    "        partition_size = int((edge_index.shape[1] + 1)/n_partitions)\n",
    "    root_queue = [0]\n",
    "    visited = []\n",
    "    partitions = []\n",
    "    root_parrent = [0]\n",
    "    \n",
    "    while root_queue:\n",
    "        root = root_queue.pop(0)\n",
    "        parrent = root_parrent.pop(0)\n",
    "        partition, queue, visited = BFS(edge_index, root, visited, partition_size)\n",
    "        root_queue += queue\n",
    "        root_parrent += [len(partitions)] * len(queue)\n",
    "        if len(partition) >= 0.5*partition_size:\n",
    "            partitions.append(partition)\n",
    "        else:\n",
    "            partitions[parrent] += partition\n",
    "    return partitions\n",
    "\n",
    "        \n",
    "parts = BFS_partition(data.edge_index.numpy(), n_parts=5000)\n",
    "parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[10]\n",
    "from preprocessing.batching import get_batch_graphs\n",
    "batch_graphs = get_batch_graphs(data=data, batch_size=None, batch_n_times=100, recursive=False)\n",
    "count = 0\n",
    "for graph in batch_graphs:\n",
    "    count += data.x.size()[0] - data.edge_attr.size()[0]\n",
    "# print(count, len(batch_graphs))\n",
    "print(data)\n",
    "print(len(batch_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merge data\n",
    "datas = batch_graphs\n",
    "# print(datas[0]._store)\n",
    "# x = torch.tensor([])\n",
    "# edge_index = torch.tensor([])\n",
    "# edge_attr = torch.tensor([])\n",
    "# for data in datas:\n",
    "#     _edge_index = data.edge_index + x.size(0)\n",
    "#     print(_edge_index)\n",
    "#     edge_index = torch.cat([edge_index, _edge_index], dim=1)\n",
    "#     x = torch.cat([x, data.x], dim=0)\n",
    "#     edge_attr = torch.cat([edge_attr, data.edge_attr], dim=0)\n",
    "\n",
    "from typing import List\n",
    "from data.data import TorchGraphData\n",
    "def merge_graphs(datas : List[TorchGraphData]):\n",
    "    keys = list(datas[0]._store.keys())\n",
    "    data_dict = {}\n",
    "    for key in keys:\n",
    "        data_dict[key] = []\n",
    "    node_count = 0\n",
    "    for data in datas:\n",
    "        data_dict['edge_index'].append(data.edge_index + node_count)\n",
    "        for key in keys:\n",
    "            if key == 'edge_index':\n",
    "                continue\n",
    "            data_dict[key].append(data._store[key])\n",
    "        node_count += data.x.size(0)\n",
    "    merged_data = TorchGraphData()\n",
    "    for key in data_dict:\n",
    "        if key == 'edge_index':\n",
    "            setattr(merged_data, key, torch.cat(data_dict[key], dim=1))\n",
    "        else:\n",
    "            setattr(merged_data, key, torch.cat(data_dict[key], dim=0))\n",
    "    return merged_data\n",
    "\n",
    "data = merge_graphs(datas)\n",
    "data\n",
    "# print(x.size(), edge_index.size(), edge_attr.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test graph partition\n",
    "data = _dataset[0]\n",
    "_get_graph_partition(\n",
    "    data=data,\n",
    "    partition=list(range(10,35)),\n",
    "    recursive=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
