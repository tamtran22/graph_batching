{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data.dataset import DatasetLoader, OneDDatasetLoader, OneDDatasetBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "# from preprocessing.batching import _get_graph_partition\n",
    "from networks.loss import *\n",
    "from preprocessing.normalize import normalize_graph\n",
    "from networks.network_parc import PARC\n",
    "from data.file_reader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start calculating derivative..\n",
      "Finish calculating derivative.\n",
      "Start normalizing..\n",
      "Finish normalizing.\n",
      "Start batching..\n",
      "Finish batching.\n"
     ]
    }
   ],
   "source": [
    "# process dataset all\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_v2'\n",
    ")\n",
    "\n",
    "print('Start calculating derivative..')\n",
    "dataset = dataset.calculate_derivative(\n",
    "    sub_dir='/calculated/',\n",
    "    var_name=['pressure', 'flowrate'],\n",
    "    axis=1,\n",
    "    delta_t=0.02\n",
    ")\n",
    "print('Finish calculating derivative.')\n",
    "\n",
    "print('Start normalizing..')\n",
    "dataset = dataset.normalizing(\n",
    "    sub_dir='/normalized/'\n",
    ")\n",
    "print('Finish normalizing.')\n",
    "\n",
    "print('Start batching..')\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_v2',\n",
    "    sub_dir='/normalized/'\n",
    ")\n",
    "val_dataset = dataset.batching(batch_size=None, batch_n_times=None, recursive=True,\n",
    "                            sub_dir='/validation/', step=5)\n",
    "\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_v2',\n",
    "    sub_dir='/normalized/'\n",
    ")\n",
    "dataset = dataset.batching(batch_size=100, batch_n_times=201, recursive=True,\n",
    "                            sub_dir='/batched/', step=5)\n",
    "\n",
    "print('Finish batching.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "TorchGraphDataBatch(edge_index=[2, 6008], node_attr=[6058, 10], pressure=[6058, 41], flowrate=[6058, 41], flowrate_bc=[6058, 41], pressure_dot=[6058, 41], flowrate_dot=[6058, 41], node_weight=[6058], batch=[6058], ptr=[51])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=50)\n",
    "print(loader.__len__())\n",
    "print(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(loader))\n",
    "data.node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_v2',\n",
    "    sub_dir='/normalized/'\n",
    ")\n",
    "y = dataset[0].flowrate_bc[100,:]\n",
    "print(y.min(), y.max())\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(y)\n",
    "# plt.ylim(-1,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_transformed[0]\n",
    "# print(data)\n",
    "graph_part = _get_graph_partition(data=data, partition=np.arange(5,15), recursive=True)\n",
    "# print(graph_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching dataset\n",
    "# batched_dataset = dataset_transformed.batching(batch_size=5000, batch_n_times=10, recursive=True,\n",
    "#                                     sub_dir='/batched/')\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_all',\n",
    "    sub_dir='/calculated/'\n",
    ")\n",
    "\n",
    "# vel_min = dataset.min('velocity')\n",
    "vel_max = dataset.max('velocity').item()\n",
    "vel_mean = dataset.mean('velocity').item()\n",
    "vel_med = dataset.median('velocity').item()\n",
    "# vel_dot_min = dataset.min('velocity_dot')\n",
    "# vel_dot_max = dataset.max('velocity_dot')\n",
    "\n",
    "# C = np.trunc(-np.log(np.abs(vel_max - vel_mean))) - 1\n",
    "\n",
    "i = 50\n",
    "# y = dataset[0].velocity_dot\n",
    "# plt.plot(y)\n",
    "z = dataset[0].velocity[:,i]\n",
    "z = logarithmic_scaler(z, mean=vel_med, log_scale=10**12)\n",
    "# z = -1 + 2*(z - z.min())/(z.max()-z.min())\n",
    "\n",
    "plt.plot(z)\n",
    "# plt.ylim([0,1e8])\n",
    "plt.show()\n",
    "plt.hist(z, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming/scaling dataset\n",
    "dataset = dataset\n",
    "p_min = dataset.min(var_name='pressure', axis=None).numpy()\n",
    "p_max = dataset.max(var_name='pressure', axis=None).numpy()\n",
    "print(p_min, p_max)\n",
    "q_min = dataset.min(var_name='flowrate', axis=None).numpy()\n",
    "q_max = dataset.max(var_name='flowrate', axis=None).numpy()\n",
    "print(q_min, q_max)\n",
    "u_min = dataset.min(var_name='velocity', axis=None).numpy()\n",
    "u_max = dataset.max(var_name='velocity', axis=None).numpy()\n",
    "print(u_min, u_max)\n",
    "\n",
    "ea_min = dataset.min(var_name='edge_attr', axis=0).numpy()\n",
    "ea_max = dataset.max(var_name='edge_attr', axis=0).numpy()\n",
    "vol0_min = ea_min[-2]\n",
    "vol1_min = ea_min[-1]\n",
    "vol0_max = ea_max[-2]\n",
    "vol1_max = ea_max[-1]\n",
    "ea_min[-2] = min(vol0_min, vol1_min)\n",
    "ea_min[-1] = min(vol0_min, vol1_min)\n",
    "ea_max[-2] = max(vol0_max, vol1_max)\n",
    "ea_max[-1] = max(vol0_max, vol1_max)\n",
    "print(ea_min, ea_max)\n",
    "\n",
    "\n",
    "# p_mean = dataset.mean(var_name='pressure', axis=None).numpy()\n",
    "# p_std = dataset.std(var_name='pressure', axis=None).numpy()\n",
    "# print(p_mean, p_std)\n",
    "# q_mean = dataset.mean(var_name='flowrate', axis=None).numpy()\n",
    "# q_std = dataset.std(var_name='flowrate', axis=None).numpy()\n",
    "# print(q_mean, q_std)\n",
    "# u_mean = dataset.mean(var_name='velocity', axis=None).numpy()\n",
    "# u_std = dataset.std(var_name='velocity', axis=None).numpy()\n",
    "# print(u_mean, u_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting pressure distribution\n",
    "# print(__dataset[0])\n",
    "y = __dataset[0].pressure.numpy()[:,50]\n",
    "\n",
    "# p = -1+2*(p-p_min)/(p_max-p_min)\n",
    "# p = (p-p_mean)/(p_std+1e-10)\n",
    "plt.ylim(-1,1)\n",
    "# plt.hist(p, bins=1000)\n",
    "plt.plot(y)\n",
    "# plt.ylim(-1,1)\n",
    "plt.show()\n",
    "plt.hist(y, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting flowrate distribution\n",
    "\n",
    "q = dataset[0].flowrate.numpy()[50000,:]\n",
    "\n",
    "# q = (q-q_mean)/(q_std+1e-10)\n",
    "# q = -1+2*(q-q_min)/(q_max-q_min)\n",
    "# plt.xlim(-1,1)\n",
    "# plt.hist(q, bins=10)\n",
    "plt.plot(list(range(len(q))), q)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume\n",
    "vol0_raw = dataset_raw[0].edge_attr[:,-2].numpy()\n",
    "vol0_transformed = dataset_transformed[0].edge_attr[:,-1].numpy()\n",
    "print(vol0_raw.shape, vol0_transformed.shape)\n",
    "print(vol0_raw[1000], vol0_transformed[1000])\n",
    "\n",
    "print(vol0_raw.mean(), vol0_raw.std())\n",
    "print(vol0_transformed.mean(), vol0_transformed.std())\n",
    "\n",
    "# v_min = min(vol0.min(), vol1.min())\n",
    "# v_max = max(vol0.max(), vol1.max())\n",
    "\n",
    "# vol0 = -1+2*(vol0 - v_min)/(v_max - v_min)\n",
    "# vol1 = -1+2*(vol1 - v_min)/(v_max - v_min)\n",
    "\n",
    "# vol0 = (vol0 - vol0.mean())/(vol0.std() + 1e-10)\n",
    "# vol1 = (vol1 - vol1.mean())/(vol1.std() + 1e-10)\n",
    "\n",
    "# vol0 = (vol0 - vol0.min())/(vol0.max() + vol0.min())\n",
    "# vol1 = (vol1 - vol1.min())/(vol1.max() + vol0.min())\n",
    "\n",
    "\n",
    "# vol0 = (vol0 - np.median(vol0))/(np.percentile(vol0, 75) - np.percentile(vol0, 25))\n",
    "# vol1 = (vol1 - np.median(vol1))/(np.percentile(vol1, 75) - np.percentile(vol1, 25))\n",
    "\n",
    "# plt.plot(list(range(vol0.shape[0])), vol0, c='red')\n",
    "# plt.plot(list(range(vol0.shape[0])), vol1, c='blue')\n",
    "# plt.xlim(4350, 4450)\n",
    "# plt.ylim(-0.98,-0.88)\n",
    "\n",
    "plt.plot(vol0_raw)\n",
    "# plt.plot(vol0_transformed)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation scatter plot\n",
    "data = dataset[0]\n",
    "x = data.edge_attr.numpy()[:,6]\n",
    "# y = data.velocity.numpy()[:,50]\n",
    "y = data.flowrate.numpy().mean(axis=1)\n",
    "\n",
    "y = (y - u_min)/(u_max - u_min)\n",
    "# y = np.sign(y)*np.log(1. + np.abs(y)/1e2)\n",
    "# d = data.edge_attr.numpy()[:,1]\n",
    "# y = y / (np.square(d))\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test normalize\n",
    "# data = dataset[0]\n",
    "# print(data)\n",
    "# data = normalize_graph(\n",
    "#     data=data,\n",
    "#     # x_min=0, x_max=512,\n",
    "#     edge_attr_min=ea_min, edge_attr_max=ea_max,\n",
    "#     pressure_min=p_min, pressure_max=p_max,\n",
    "#     velocity_min=u_min, velocity_max=u_max\n",
    "# )\n",
    "# print(data)\n",
    "# plt.plot(data.flowrate_bc[1000,:])\n",
    "# plt.show()\n",
    "\n",
    "# y_true = data.velocity[:,8]\n",
    "# y_pred = data.velocity[:,9]\n",
    "# print(y_true, y_pred, data.weight)\n",
    "# plt.plot(y_true)\n",
    "# plt.plot(y_pred)\n",
    "# plt.show()\n",
    "\n",
    "# loss = weighted_mean_squared_error(y_pred, y_true, torch.tensor(data.weight))\n",
    "# print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weighted loss\n",
    "data = normed_data\n",
    "diam = data.edge_attr.numpy()[:,0]\n",
    "length = data.edge_attr.numpy()[:,1]\n",
    "\n",
    "(count, bins) = np.histogram(diam, bins=5000)\n",
    "n_edge = diam.shape[0]\n",
    "\n",
    "# print(count)\n",
    "# print(bins)\n",
    "\n",
    "def weight(val : float):\n",
    "    bin_id = np.where(bins >= val)[0][0] - 1\n",
    "    weight_mean = 1. / (count[bin_id] + 1e0)\n",
    "    return weight_mean\n",
    "\n",
    "v_weight = np.vectorize(weight)\n",
    "weight_diam = v_weight(diam)\n",
    "\n",
    "\n",
    "def cal_weight(x : np.array, bins=1000) -> np.array:\n",
    "    (count, bin) = np.histogram(x, bins=bins)\n",
    "    N = x.shape[0]\n",
    "\n",
    "    def _weight(value : float):\n",
    "        _bin_id = np.where(bin >= value)[0][0] - 1\n",
    "        _weight = 1. / (count[_bin_id] + 1.)\n",
    "        return _weight\n",
    "    \n",
    "    v_weight = np.vectorize(_weight)\n",
    "\n",
    "    return v_weight(x)\n",
    "\n",
    "plt.plot(bins[:-1], count)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(weight_diam)\n",
    "# plt.xlim(21000,21025)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batching BFS\n",
    "import random\n",
    "data = dataset[0]\n",
    "def BFS_partition(edge_index, partition_size=None, n_partitions=None):\n",
    "    def BFS(edge_index, root, visited, part_size=100):\n",
    "        queue = [root]\n",
    "        visited = []\n",
    "        part = []\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            while current in visited:\n",
    "                current = queue.pop(0)\n",
    "            visited.append(current)\n",
    "            part.append(current)\n",
    "            # find child nodes\n",
    "            child_edges = np.where(edge_index[0] == current)[0]\n",
    "            child_nodes = list(edge_index[1][child_edges])\n",
    "            # add child nodes to queue\n",
    "            queue += child_nodes\n",
    "            # break\n",
    "            if len(part) >= part_size:\n",
    "                break\n",
    "        return part, queue, visited\n",
    "    \n",
    "    if partition_size is None:\n",
    "        partition_size = int((edge_index.shape[1] + 1)/n_partitions)\n",
    "    root_queue = [0]\n",
    "    visited = []\n",
    "    partitions = []\n",
    "    root_parrent = [0]\n",
    "    \n",
    "    while root_queue:\n",
    "        root = root_queue.pop(0)\n",
    "        parrent = root_parrent.pop(0)\n",
    "        partition, queue, visited = BFS(edge_index, root, visited, partition_size)\n",
    "        root_queue += queue\n",
    "        root_parrent += [len(partitions)] * len(queue)\n",
    "        if len(partition) >= 0.5*partition_size:\n",
    "            partitions.append(partition)\n",
    "        else:\n",
    "            partitions[parrent] += partition\n",
    "    return partitions\n",
    "\n",
    "        \n",
    "parts = BFS_partition(data.edge_index.numpy(), n_parts=5000)\n",
    "parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[10]\n",
    "from preprocessing.batching import get_batch_graphs\n",
    "batch_graphs = get_batch_graphs(data=data, batch_size=None, batch_n_times=100, recursive=False)\n",
    "count = 0\n",
    "for graph in batch_graphs:\n",
    "    count += data.x.size()[0] - data.edge_attr.size()[0]\n",
    "# print(count, len(batch_graphs))\n",
    "print(data)\n",
    "print(len(batch_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test merge data\n",
    "datas = batch_graphs\n",
    "# print(datas[0]._store)\n",
    "# x = torch.tensor([])\n",
    "# edge_index = torch.tensor([])\n",
    "# edge_attr = torch.tensor([])\n",
    "# for data in datas:\n",
    "#     _edge_index = data.edge_index + x.size(0)\n",
    "#     print(_edge_index)\n",
    "#     edge_index = torch.cat([edge_index, _edge_index], dim=1)\n",
    "#     x = torch.cat([x, data.x], dim=0)\n",
    "#     edge_attr = torch.cat([edge_attr, data.edge_attr], dim=0)\n",
    "\n",
    "from typing import List\n",
    "from data.data import TorchGraphData\n",
    "def merge_graphs(datas : List[TorchGraphData]):\n",
    "    keys = list(datas[0]._store.keys())\n",
    "    data_dict = {}\n",
    "    for key in keys:\n",
    "        data_dict[key] = []\n",
    "    node_count = 0\n",
    "    for data in datas:\n",
    "        data_dict['edge_index'].append(data.edge_index + node_count)\n",
    "        for key in keys:\n",
    "            if key == 'edge_index':\n",
    "                continue\n",
    "            data_dict[key].append(data._store[key])\n",
    "        node_count += data.x.size(0)\n",
    "    merged_data = TorchGraphData()\n",
    "    for key in data_dict:\n",
    "        if key == 'edge_index':\n",
    "            setattr(merged_data, key, torch.cat(data_dict[key], dim=1))\n",
    "        else:\n",
    "            setattr(merged_data, key, torch.cat(data_dict[key], dim=0))\n",
    "    return merged_data\n",
    "\n",
    "data = merge_graphs(datas)\n",
    "data\n",
    "# print(x.size(), edge_index.size(), edge_attr.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test graph partition\n",
    "data = _dataset[0]\n",
    "_get_graph_partition(\n",
    "    data=data,\n",
    "    partition=list(range(10,35)),\n",
    "    recursive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative\n",
    "def cal_derivative(F : torch.Tensor, dim : int = -1, delta_t : float = 1.) -> torch.Tensor:\n",
    "    _F = F.transpose(0, dim)\n",
    "    deriv_F = []\n",
    "    for i in range(1, _F.size(0)):\n",
    "        deriv_F_i = (_F[i] - _F[i-1]) / delta_t\n",
    "        deriv_F.append(deriv_F_i.unsqueeze(dim))\n",
    "    return torch.cat(deriv_F, dim=dim)\n",
    "\n",
    "data = dataset[0]\n",
    "Fs_hat = data.velocity.unsqueeze(-1)\n",
    "F_dots_hat = cal_derivative(Fs_hat, dim=1, delta_t=0.02)\n",
    "Fs_hat = Fs_hat[:,1:,:]\n",
    "\n",
    "plt.plot(F_dots_hat[50000, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_thao'\n",
    ")\n",
    "print(dataset.data_names)\n",
    "\n",
    "import numpy as np\n",
    "def group_by(x, batch):\n",
    "    labels = np.unique(batch)\n",
    "    mean_labels = np.zeros_like(labels)\n",
    "    std_labels = np.zeros_like(labels)\n",
    "    for label in labels:\n",
    "        label_index = np.where(batch == label)[0]\n",
    "        x_label = x[label_index]\n",
    "        mean_labels[label] = np.mean(x_label)\n",
    "        std_labels[label] = np.std(x_label)\n",
    "    return labels, mean_labels, std_labels\n",
    "\n",
    "x_dl = dataset[0].pressure[:,150].numpy()\n",
    "batch = dataset[0].node_attr[:,5].numpy().astype(int)\n",
    "label_dl, x_label_dl, std_label_dl = group_by(x_dl, batch)\n",
    "\n",
    "x_tb = dataset[1].pressure[:,150].numpy()\n",
    "batch = dataset[1].node_attr[:,5].numpy().astype(int)\n",
    "label_tb, x_label_tb, std_label_tb = group_by(x_tb, batch)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.bar(label - 0.2, -x_label_vd, 0.4, color='tab:blue', label='Taubin')\n",
    "# plt.bar(label + 0.2, -x_label_dl, 0.4, color='tab:orange', label='AMSL')\n",
    "\n",
    "# plt.plot(x_label_dl, color='tab:orange', label='AMSL')\n",
    "# plt.plot(x_label_tb, color='tab:blue', label='Taubin')\n",
    "\n",
    "plt.errorbar(label_tb, x_label_tb, std_label_tb, label='Taubin',\n",
    "            fmt='g-', capsize=3, color='tab:blue', errorevery=2)\n",
    "plt.errorbar(label_dl, x_label_dl, std_label_dl, label='AMSL', \n",
    "            fmt='g-', capsize=3, color='tab:orange', errorevery=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Generations', size=12)\n",
    "plt.ylabel('Pressure [Pa]', size=12)\n",
    "# plt.xticks([0, 1, 2, 3, 4, 5], ['Central', 'RUL', 'RML', 'RLL', 'LUL', 'LLL'])\n",
    "plt.xticks([0,2,4,6,8,10,12,14,16,18,20,22,24])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss, _WeightedLoss\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "def weighted_mean_squared_error(input, target, weight):\n",
    "    squared_error = torch.square(input - target)\n",
    "    weighted_square_error = torch.einsum('ijk,i->ijk', squared_error, weight)\n",
    "    return torch.mean(weighted_square_error)\n",
    "\n",
    "class WeightedMSELoss(_Loss):\n",
    "    def __init__(self, size_average=None, \n",
    "                reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "    def forward(self, input : Tensor, target : Tensor, weight : Tensor):\n",
    "        return weighted_mean_squared_error(input, target, weight)\n",
    "\n",
    "y_pred = torch.ones((10460, 10, 2))\n",
    "y_true = torch.ones((10460, 10, 2))\n",
    "weight = torch.ones((10460,))\n",
    "\n",
    "criterion = WeightedMSELoss()\n",
    "\n",
    "loss = criterion(y_true, y_pred, weight)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[3,4], [5,6]])\n",
    "b = torch.tensor([1,2,3])\n",
    "\n",
    "torch.einsum('ij,i->ij', a, b).size()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
