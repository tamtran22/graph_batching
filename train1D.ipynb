{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "from networks.network import EmbeddedNet\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from preprocessing.dataset import OneDGraphDataset\n",
    "from preprocessing.batching import metis_batching_torch\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to load 1D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_id = [str(i).zfill(3) for i in range(201)]\n",
    "dataset = OneDGraphDataset(\n",
    "    raw_file_dir='/data1/tam/datasets',\n",
    "    root_dir='/data1/tam/downloaded_datasets',\n",
    "    data_names='all',\n",
    "    time_id=time_id,\n",
    "    transform=None,\n",
    "    pre_transform=None,\n",
    "    pre_filter=None,\n",
    "    is_loader=False\n",
    ")\n",
    "print(dataset.data_names[10])\n",
    "print(dataset.len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_percent = 0.8\n",
    "case_name = dataset.data_names\n",
    "# case_name = sorted(np.unique([name[4:7] for name in case_name]))\n",
    "# print(case_name)\n",
    "train_id = range(0, 31)\n",
    "eval_id = range(31, 35)\n",
    "val_id = range(35, 41)\n",
    "print(f\"Load dataset, split with {len(train_id)} training cases and {len(eval_id)} evaluating cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-7\n",
    "decay = 5e-4\n",
    "\n",
    "model = EmbeddedNet(\n",
    "    input_dim_node=1, \n",
    "    input_dim_edge=5, \n",
    "    hidden_dim=128, \n",
    "    output_dim=1,  \n",
    "    num_processor_layers = 10, \n",
    "    emb=False, \n",
    "    add_self_loops=True\n",
    ")\n",
    "model = model.to(device)\n",
    "# model = torch.nn.DataParallel(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "print(f'Loaded model: {model}')\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.HuberLoss(delta=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, f_prev, f, p_prev, p, edge_index, edge_attr):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    print(x.size(), f_prev.size(), p_prev.size(), edge_attr.size(), edge_index.size())\n",
    "    p_pred, f_pred = model(x.float(), f_prev.float(), p_prev.float(), edge_index, edge_attr.float())\n",
    "    loss = criterion(f_pred, f.float())  + criterion(p_pred, p.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def eval(x, f_prev, f, p_prev, p, edge_index, edge_attr):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p_pred, f_pred = model(x.float(), f_prev.float(), p_prev.float(), edge_index, edge_attr.float())\n",
    "        loss = criterion(f_pred, f.float()) + criterion(p_pred, p.float())\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(dataset.len):\n",
    "    data += metis_batching_torch(dataset[i], relative_batch_size=100, recursive=True)\n",
    "    print(f'Finish data number {i}.')\n",
    "train_id = range(0, 15000)\n",
    "eval_id = range(15000, 24000)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "print('Start training.')\n",
    "train_loss_all = []\n",
    "eval_loss_all = []\n",
    "for epoch in range(100):\n",
    "    train_loss = 0.0\n",
    "    for i in train_id:\n",
    "        x = data[i].x #.to(device)\n",
    "        edge_index = data[i].edge_index #.to(device)\n",
    "        edge_attr = data[i].edge_attr #.to(device)\n",
    "        for time in range(1, len(time_id)):\n",
    "            p_prev = data[i].pressure[:,time - 1].unsqueeze(1) #.to(device)\n",
    "            p = data[i].pressure[:,time].unsqueeze(1) #.to(device)\n",
    "            f_prev = data[i].flowrate[:,time - 1].unsqueeze(1) #.to(device)\n",
    "            f = data[i].flowrate[:,time].unsqueeze(1) #.to(device)\n",
    "            # print(f_prev.size(), edge_attr.size())\n",
    "            train_loss += train(x, f_prev, f, p_prev, p, edge_index, edge_attr)\n",
    "    train_loss /= len(train_id)\n",
    "    train_loss_all.append(train_loss)\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    for i in eval_id:\n",
    "        x_eval = data[i].x\n",
    "        edge_index_eval = data[i].edge_index\n",
    "        edge_attr_eval = data[i].edge_attr\n",
    "        for time in range(1, len(time_id)):\n",
    "            p_prev_eval = data[i].pressure[:,time - 1].unsqueeze(1)\n",
    "            p_eval = data[i].pressure[:,time].unsqueeze(1)\n",
    "            f_prev_eval = data[i].flowrate[:,time - 1].unsqueeze(1)\n",
    "            f_eval = data[i].flowrate[:,time].unsqueeze(1)\n",
    "            eval_loss += eval(x_eval, f_prev_eval, f_eval, p_prev_eval, p_eval,\\\n",
    "                        edge_index_eval, edge_attr_eval)\n",
    "    eval_loss /= len(eval_id)\n",
    "    eval_loss_all.append(eval_loss)\n",
    "    \n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch}: train loss={train_loss}; eval loss={eval_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/mgn_v3.pth')\n",
    "plt.plot(train_loss_all, label='train_loss')\n",
    "plt.plot(eval_loss_all, label='eval_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "f = open('./loss_mgn_v3','w+')\n",
    "for i in range(len(train_loss_all)):\n",
    "    f.write(str(train_loss_all[i]))\n",
    "    f.write('    ')\n",
    "    f.write(str(eval_loss_all[i]))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "out_dir = './predict_v3/'\n",
    "model = EmbeddedNet(\n",
    "    input_dim_node=4, \n",
    "    input_dim_edge=1, \n",
    "    hidden_dim=128, \n",
    "    output_dim=1,  \n",
    "    num_processor_layers = 10, \n",
    "    emb=False, \n",
    "    add_self_loops=True\n",
    ")\n",
    "device = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load('models/mgn_v3.pth'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def print_1D(x, edge_index, aoa, left_elem, right_elem, num_elem, case, path):\n",
    "    fo = open(path+'/output_'+case+'.dat','w+')\n",
    "    fo.write(f'VARIABLES=\"CoordinateX\"\\n\"CoordinateY\"\\n\"CoordinateZ\"\\n\"flag\"\\n\"uds-0-scalar\"\\n')\n",
    "    fo.write(f'ZONE T= \"output_iso_{case}.dat\"\\n')\n",
    "    fo.write('STRANDID=17, SOLUTIONTIME=0\\n')\n",
    "    fo.write(f'Nodes={np.shape(x)[0]}, Faces={np.shape(edge_index)[1]}, Elements={num_elem}, ZONETYPE=FEPolygon\\n')\n",
    "    fo.write(f'DATAPACKING=BLOCK\\nNumConnectedBoundaryFaces=0, TotalNumBoundaryConnections=0\\nAUXDATA Time=\"0.000000e+00\"\\nDT=(SINGLE SINGLE SINGLE SINGLE SINGLE )\\n')\n",
    "    line_count = 0\n",
    "    for j in range(np.shape(x)[1]):\n",
    "        for i in range(np.shape(x)[0]):\n",
    "            fo.write(f' {x[i][j]}')\n",
    "            line_count += 1\n",
    "            if line_count == 5:\n",
    "                fo.write('\\n')\n",
    "                line_count = 0\n",
    "        if line_count != 0:\n",
    "            fo.write('\\n')\n",
    "        line_count = 0\n",
    "    for i in range(np.shape(aoa)[0]):\n",
    "        fo.write(f' {aoa[i]}')\n",
    "        line_count += 1\n",
    "        if line_count == 5:\n",
    "            fo.write('\\n')\n",
    "            line_count = 0\n",
    "    if line_count != 0:\n",
    "        fo.write('\\n')\n",
    "    line_count = 0\n",
    "    \n",
    "    fo.write('# face nodes\\n')\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        fo.write(f' {edge_index[0][i]+1}\\t{edge_index[1][i]+1}')\n",
    "        line_count += 1\n",
    "        if line_count == 5:\n",
    "            fo.write('\\n')\n",
    "            line_count = 0\n",
    "    if line_count != 0:\n",
    "        fo.write('\\n')\n",
    "    line_count = 0\n",
    "\n",
    "    fo.write('# left elements\\n')\n",
    "    for i in range(left_elem.shape[0]):\n",
    "        fo.write(f' {left_elem[i]}')\n",
    "        line_count += 1\n",
    "        if line_count == 10:\n",
    "            fo.write('\\n')\n",
    "            line_count = 0\n",
    "    if line_count != 0:\n",
    "        fo.write('\\n')\n",
    "    line_count = 0\n",
    "\n",
    "    fo.write('# right elements\\n')\n",
    "    for i in range(right_elem.shape[0]):\n",
    "        fo.write(f' {right_elem[i]}')\n",
    "        line_count += 1\n",
    "        if line_count == 10:\n",
    "            fo.write('\\n')\n",
    "            line_count = 0\n",
    "    if line_count != 0:\n",
    "        fo.write('\\n')\n",
    "    line_count = 0\n",
    "    fo.close()\n",
    "import os\n",
    "os.system('rm -rf {out_dir}*')\n",
    "os.system('mkdir '+out_dir)\n",
    "f = open('mean_aoa.dat','w+')\n",
    "mean_aoa_true = []\n",
    "mean_aoa_pred = []\n",
    "for i in eval_id:\n",
    "    x = data[i].x.to(device)\n",
    "    edge_index = data[i].edge_index.to(device)\n",
    "    edge_attr = data[i].edge_attr.to(device)\n",
    "    aoa = model(x.float(), edge_index, edge_attr.float())\n",
    "    left_elem = data[i].left_element.cpu().numpy().astype(np.int32)\n",
    "    right_elem = data[i].right_element.cpu().numpy().astype(np.int32)\n",
    "    aoa = reverse_normalize(aoa.squeeze(1).detach().numpy(), mean_aoa, std_aoa)\n",
    "    print_1D(\n",
    "        x = dataset[i].x.cpu().numpy(), \n",
    "        edge_index=dataset[i].edge_index.cpu().numpy().astype(np.int32),\n",
    "        aoa= aoa,\n",
    "        left_elem=left_elem,\n",
    "        right_elem=right_elem,\n",
    "        num_elem=dataset[i].num_elem,\n",
    "        case=dataset.data_names[i], \n",
    "        path=out_dir\n",
    "    )\n",
    "    batch = torch.zeros(size=(x.size()[0],)).type(torch.LongTensor).to(device)\n",
    "    mean_aoa_true.append(global_mean_pool(dataset[i].age_of_air.squeeze(1), batch)[0])\n",
    "    mean_aoa_pred.append(global_mean_pool(torch.tensor(aoa), batch)[0])\n",
    "    f.write(f'{global_mean_pool(dataset[i].age_of_air.squeeze(1), batch)[0]}\\t{global_mean_pool(torch.tensor(aoa), batch)[0]}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sur_id in range(0, 2):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(len(eval_id)):\n",
    "        if i % 2 == sur_id:\n",
    "            y_true.append(mean_aoa_true[i])\n",
    "            y_pred.append(mean_aoa_pred[i])\n",
    "\n",
    "    # print(floor_mean_aoa_pred)\n",
    "    _x = range(len(y_true))\n",
    "    plt.plot(_x, y_true, label='true'+str(sur_id))\n",
    "    plt.plot(_x, y_pred, label='pred'+str(sur_id))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time his\n",
    "i = 35\n",
    "print(dataset.subject_list[i])\n",
    "path = out_dir+dataset.subject_list[i]+'/'\n",
    "# path_ref = './predict_2/'+dataset.subject_list[i]+'/'\n",
    "node_id = 50000\n",
    "time_list = [i * 4.8 / 200 for i in range(201)]\n",
    "def read_value(path, node_id):\n",
    "    f = open(path, 'r')\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    for i in range(node_id + 1):\n",
    "        line = f.readline()\n",
    "    line = line[:-1].split('\\t')\n",
    "    f.close()\n",
    "    return float(line[-2])\n",
    "value = []\n",
    "value_ref = []\n",
    "for time in time_id:\n",
    "    value.append(read_value(path+'plt_nd_000'+time+'.dat',node_id))\n",
    "    # value_ref.append(read_value(path_ref+'plt_nd_000'+time+'.dat',node_id))\n",
    "y_pred = np.array(value)\n",
    "# y_ref = np.array(value_ref)\n",
    "y_true = dataset[i].p[node_id].numpy()\n",
    "# plt.plot(time_list, y_ref, c='black', label='GCN')\n",
    "plt.plot(time_list, y_pred, c='red', label='ResGCN')\n",
    "plt.plot(time_list, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "# plt.ylim([-50,50])\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Pressure', fontsize=20)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db637ecd267e8785137d274d569a1de945f5091a2538863d3f2fda8d3b4d9cce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
