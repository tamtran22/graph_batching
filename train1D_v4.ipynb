{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data.dataset import DatasetLoader\n",
    "from data.data import TorchGraphData\n",
    "from preprocessing.normalize import *\n",
    "from preprocessing.batching_v2 import get_batch_graphs\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "def flowrate_bc_sin(flowrate, n_edge : int, \n",
    "                    n_time : int)->torch.Tensor:\n",
    "    _flowrate = []\n",
    "    T = 4.8 # seconds\n",
    "    for i in range(n_time):\n",
    "        t = i * T / (n_time - 1)\n",
    "        value = flowrate(t)\n",
    "        _flowrate.append(torch.full(size=(n_edge, 1), fill_value=value))\n",
    "    return torch.cat(_flowrate, dim=1)\n",
    "        \n",
    "\n",
    "def normalize(data, mean_ea, std_ea):\n",
    "    x = min_max_scaler(data.x, min=0, max=500)\n",
    "    edge_index = data.edge_index\n",
    "    edge_attr = standard_scaler(data.edge_attr,\n",
    "                mean=mean_ea,\n",
    "                std=std_ea)\n",
    "    pressure = data.pressure*1e-3\n",
    "    velocity = data.velocity*1e-3\n",
    "    f = lambda t : (np.pi * 1e0 / 4.8) * math.sin(2 * np.pi * t / 4.8)\n",
    "    flowrate_bc = flowrate_bc_sin(flowrate=f, n_edge=data.edge_attr.size(0),\\\n",
    "                                n_time=data.flowrate.size(1))\n",
    "    return TorchGraphData(x=x,edge_index=edge_index,edge_attr=edge_attr,\n",
    "                        pressure=pressure, velocity=velocity, \n",
    "                        flowrate_bc=flowrate_bc)\n",
    "\n",
    "def train_eval_split(dataset : DatasetLoader, train_id : List, eval_id : List,\n",
    "                    batch_size : int, batch_n_times : int, recursive : bool):\n",
    "    mean, std = mean_std_dataset(dataset, set_id=list(range(dataset.len())))\n",
    "    # Train dataset\n",
    "    train_dataset = []\n",
    "    for i in train_id:\n",
    "        data = normalize(dataset[i], mean, std)\n",
    "        train_dataset += get_batch_graphs(data=data, batch_size=batch_size,\n",
    "                        batch_n_times=batch_n_times, recursive=recursive)\n",
    "    # Test dataset\n",
    "    eval_dataset = []\n",
    "    for i in eval_id:\n",
    "        data = normalize(dataset[i], mean, std)\n",
    "        eval_dataset += get_batch_graphs(data=data, batch_size=batch_size,\n",
    "                        batch_n_times=batch_n_times, recursive=recursive)\n",
    "        # eval_dataset += [data]\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "def mean_std_dataset(dataset : DatasetLoader, set_id : List, field : str = 'edge_attr'):\n",
    "    fields = []\n",
    "    for i in set_id:\n",
    "        fields.append(dataset[i]._store[field])\n",
    "    fields = torch.cat(fields, dim=0)\n",
    "    return fields.mean(axis=0), fields.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = DatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_transformed'\n",
    ")\n",
    "\n",
    "train_dataset, eval_dataset = train_eval_split(\n",
    "    dataset=_dataset,\n",
    "    train_id=list(range(0, 20)),\n",
    "    eval_id=list(range(20, 40)),\n",
    "    batch_size=10000,\n",
    "    batch_n_times=10,\n",
    "    recursive=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from networks.network_recurrent import RecurrentMeshGraphNet\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from networks.network_recurrent import objectview\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model params\n",
    "args = objectview({\n",
    "    'input_dim_node' : _dataset[0].x.size(1)+1,\n",
    "    'input_dim_edge' : _dataset[0].edge_attr.size(1)+2,\n",
    "    'output_dim_node' : 1,\n",
    "    'output_dim_edge' : 1,\n",
    "    'hidden_dim' : 128,\n",
    "    'n_processors' : 10,\n",
    "    'n_time' : _dataset[0].pressure.size(1),\n",
    "    'device' : torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'lr' : 1e-7,\n",
    "    'weight_decay' : 5e-2,\n",
    "    'epoch' : 100\n",
    "})\n",
    "\n",
    "# Model initializing\n",
    "model = RecurrentMeshGraphNet(\n",
    "    input_dim_node = args.input_dim_node,\n",
    "    input_dim_edge = args.input_dim_edge,\n",
    "    output_dim_node = args.output_dim_node,\n",
    "    output_dim_edge = args.output_dim_edge,\n",
    "    hidden_dim = args.hidden_dim,\n",
    "    n_processors = args.n_processors\n",
    ")\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 3.453124517261982; eval loss = 1.053575018963011\n",
      "Epoch 1: train loss = 0.8335374938994646; eval loss = 0.6691032446699567\n",
      "Epoch 2: train loss = 0.5963589655905962; eval loss = 0.5185086416209689\n",
      "Epoch 3: train loss = 0.4807620374113321; eval loss = 0.4317912969627593\n",
      "Epoch 4: train loss = 0.41476871917396785; eval loss = 0.3781255329894547\n",
      "Epoch 5: train loss = 0.3737703591771424; eval loss = 0.3422349968500952\n",
      "Epoch 6: train loss = 0.34589777915924785; eval loss = 0.3165890046275488\n",
      "Epoch 7: train loss = 0.325512070376426; eval loss = 0.29728591420346556\n",
      "Epoch 8: train loss = 0.3097923653945327; eval loss = 0.2821948009847414\n",
      "Epoch 9: train loss = 0.29722966071590784; eval loss = 0.2700064215255846\n",
      "Epoch 10: train loss = 0.2869205148629844; eval loss = 0.25990360533210016\n",
      "Epoch 11: train loss = 0.27829304997995497; eval loss = 0.25135789717003554\n",
      "Epoch 12: train loss = 0.2709647073447704; eval loss = 0.24401833539146303\n",
      "Epoch 13: train loss = 0.26466250626556576; eval loss = 0.23764138051520775\n",
      "Epoch 14: train loss = 0.2591780664008111; eval loss = 0.2320421693588395\n",
      "Epoch 15: train loss = 0.25435339429974557; eval loss = 0.22708381147741682\n",
      "Epoch 16: train loss = 0.2500634177085012; eval loss = 0.22265777675121431\n",
      "Epoch 17: train loss = 0.24621094165928661; eval loss = 0.21867651724726847\n",
      "Epoch 18: train loss = 0.2427203058935702; eval loss = 0.21507191483189564\n",
      "Epoch 19: train loss = 0.23953294207528233; eval loss = 0.21178735568197352\n",
      "Epoch 20: train loss = 0.2366048675235361; eval loss = 0.2087794660673578\n",
      "Epoch 21: train loss = 0.23390013810247182; eval loss = 0.2060102780045259\n",
      "Epoch 22: train loss = 0.231389266801998; eval loss = 0.20344753374742106\n",
      "Epoch 23: train loss = 0.22904711969196798; eval loss = 0.20106344486559086\n",
      "Epoch 24: train loss = 0.2268518125489354; eval loss = 0.1988394010646066\n",
      "Epoch 25: train loss = 0.22478581776749343; eval loss = 0.19675684936971652\n",
      "Epoch 26: train loss = 0.2228341683903709; eval loss = 0.1947995344029352\n",
      "Epoch 27: train loss = 0.22098462294694035; eval loss = 0.19295465174214085\n",
      "Epoch 28: train loss = 0.21922724274452776; eval loss = 0.1912112167787434\n",
      "Epoch 29: train loss = 0.21755214237421752; eval loss = 0.18955873357002984\n",
      "Epoch 30: train loss = 0.21595058245491236; eval loss = 0.1879871856937609\n",
      "Epoch 31: train loss = 0.21441608859226108; eval loss = 0.18648941473445238\n",
      "Epoch 32: train loss = 0.212942204114981; eval loss = 0.18505743553695997\n",
      "Epoch 33: train loss = 0.2115230873497203; eval loss = 0.18368555387334512\n",
      "Epoch 34: train loss = 0.21015372784156353; eval loss = 0.18236909312481928\n",
      "Epoch 35: train loss = 0.20883005763217807; eval loss = 0.181102863120118\n",
      "Epoch 36: train loss = 0.20754771999735386; eval loss = 0.1798829036451286\n",
      "Epoch 37: train loss = 0.20630336775351316; eval loss = 0.17870466918915068\n",
      "Epoch 38: train loss = 0.20509363972209393; eval loss = 0.177564370617418\n",
      "Epoch 39: train loss = 0.203915540616028; eval loss = 0.17646022907983844\n",
      "Epoch 40: train loss = 0.2027663752855733; eval loss = 0.17538774356725487\n"
     ]
    }
   ],
   "source": [
    "# Train function v2\n",
    "def train(model, data, args):\n",
    "    n_edge = data.edge_attr.size(0)\n",
    "    hidden = torch.zeros(n_edge, args.hidden_dim).to(args.device)\n",
    "    _x = data.x #.to(args.device)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    _edge_attr = data.edge_attr #.to(args.device)\n",
    "\n",
    "    _pressure = data.pressure[:,0].unsqueeze(1)\n",
    "    _velocity = data.velocity[:,0].unsqueeze(1)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(1, data.number_of_timesteps):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # _pressure = data.pressure[:,i-1].unsqueeze(1)\n",
    "        # _velocity = data.velocity[:,i-1].unsqueeze(1)\n",
    "        _flowrate_bc = data.flowrate_bc[:,i].unsqueeze(1)\n",
    "        x = torch.cat([_x, _pressure], dim=1).to(args.device)\n",
    "        edge_attr = torch.cat([_edge_attr, _velocity, _flowrate_bc], dim=1).to(args.device)\n",
    "        node_out, edge_out, hidden = model(x, edge_index, edge_attr, hidden)\n",
    "        hidden = hidden.detach()\n",
    "        _pressure = node_out.detach().cpu()\n",
    "        _velocity = edge_out.detach().cpu()\n",
    "\n",
    "        loss += criterion(node_out, data.pressure[:,i].unsqueeze(1).float().to(args.device))\n",
    "        loss += criterion(edge_out, data.velocity[:,i].unsqueeze(1).float().to(args.device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Eval function\n",
    "def eval(model, data, args):\n",
    "    n_edge = data.edge_attr.size(0)\n",
    "    hidden = torch.zeros(n_edge, args.hidden_dim).to(args.device)\n",
    "    _x = data.x #.to(args.device)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    _edge_attr = data.edge_attr #.to(args.device)\n",
    "\n",
    "    _pressure = data.pressure[:,0].unsqueeze(1)\n",
    "    _velocity = data.velocity[:,0].unsqueeze(1)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(1, data.number_of_timesteps):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # _pressure = data.pressure[:,i-1].unsqueeze(1)\n",
    "            # _velocity = data.velocity[:,i-1].unsqueeze(1)\n",
    "            _flowrate_bc = data.flowrate_bc[:,i].unsqueeze(1)\n",
    "            x = torch.cat([_x, _pressure], dim=1).to(args.device)\n",
    "            edge_attr = torch.cat([_edge_attr, _velocity, _flowrate_bc], dim=1).to(args.device)\n",
    "            node_out, edge_out, hidden = model(x, edge_index, edge_attr, hidden)\n",
    "            hidden = hidden.detach()\n",
    "            _pressure = node_out.detach().cpu()\n",
    "            _velocity = edge_out.detach().cpu()\n",
    "\n",
    "            loss += criterion(node_out, data.pressure[:,i].unsqueeze(1).float().to(args.device))\n",
    "            loss += criterion(edge_out, data.velocity[:,i].unsqueeze(1).float().to(args.device))\n",
    "\n",
    "    return loss.item()\n",
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "for epoch in range(args.epoch):\n",
    "    train_loss = 0\n",
    "    for data in train_dataset:\n",
    "        train_loss += train(model=model, data=data, args=args)\n",
    "    train_loss /= len(train_dataset)\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    for data in eval_dataset:\n",
    "        eval_loss += eval(model=model, data=data, args=args)\n",
    "    eval_loss /= len(eval_dataset)\n",
    "    total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'models/rmgnv1_epoch{epoch}.pth')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'models/rmgnv2_epoch100.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct CFD output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct CFD\n",
    "def print_prediction(model, data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initializing\n",
    "model = RecurrentMeshGraphNet(\n",
    "    input_dim_node = args.input_dim_node,\n",
    "    input_dim_edge = args.input_dim_edge,\n",
    "    output_dim_node = args.output_dim_node,\n",
    "    output_dim_edge = args.output_dim_edge,\n",
    "    hidden_dim = args.hidden_dim,\n",
    "    n_processors = args.n_processors\n",
    ")\n",
    "model = model.to(args.device)\n",
    "model.load_state_dict(torch.load('models/rmgnv2_epoch100.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction/ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(model, data):\n",
    "    n_edge = data.edge_attr.size(0)\n",
    "    hidden = torch.zeros(n_edge, args.hidden_dim).to(args.device)\n",
    "    _x = data.x\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    _edge_attr = data.edge_attr\n",
    "\n",
    "    total_loss = 0\n",
    "    total_node_out = [data.pressure[:,0].unsqueeze(1)]\n",
    "    total_edge_out = [data.velocity[:,0].unsqueeze(1)]\n",
    "    for i in range(1, args.n_time):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _pressure = data.pressure[:,i-1].unsqueeze(1)\n",
    "            _velocity = data.velocity[:,i-1].unsqueeze(1)\n",
    "            _flowrate_bc = data.flowrate_bc[:,i].unsqueeze(1)\n",
    "            x = torch.cat([_x, _pressure], dim=1).to(args.device)\n",
    "            edge_attr = torch.cat([_edge_attr, _velocity, _flowrate_bc], dim=1).to(args.device)\n",
    "            node_out, edge_out, hidden = model(x, edge_index, edge_attr, hidden)\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "\n",
    "            loss = criterion(node_out, data.pressure[:,i].unsqueeze(1).float().to(args.device))\n",
    "            loss += criterion(edge_out, data.velocity[:,i].unsqueeze(1).float().to(args.device))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_node_out.append(node_out.detach().cpu())\n",
    "            total_edge_out.append(edge_out.detach().cpu())\n",
    "    total_node_out = torch.cat(total_node_out, dim=1)\n",
    "    total_edge_out = torch.cat(total_edge_out, dim=1)\n",
    "    \n",
    "    # plot\n",
    "    node = 50000\n",
    "    y_pred = total_edge_out[node].numpy()\n",
    "    y_true = data.velocity[node].numpy()\n",
    "    x = [i * 4.8 /200 for i in range(201)]\n",
    "    plt.plot(x, y_pred, c='red', label='RMGN')\n",
    "    plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-50,50])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Pressure', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return total_loss\n",
    "    \n",
    "mean, std = mean_std_dataset(_dataset, set_id=list(range(_dataset.len())))\n",
    "plot_comparison(model, normalize(_dataset[40], mean, std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db637ecd267e8785137d274d569a1de945f5091a2538863d3f2fda8d3b4d9cce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
