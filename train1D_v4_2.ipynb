{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data.dataset import OneDDatasetLoader, DatasetLoader\n",
    "from typing import List\n",
    "from preprocessing.batching import merge_graphs\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def train_eval_split(dataset : DatasetLoader, train_id : List, eval_id : List):\n",
    "    # Get batching id\n",
    "    if dataset._sub_dir == '/batched/':\n",
    "        batching_id = dataset.batching_id.numpy()\n",
    "        train_id = list(np.where(np.isin(batching_id, train_id) == True)[0])\n",
    "        eval_id = list(np.where(np.isin(batching_id, eval_id) == True)[0])\n",
    "    # Train dataset\n",
    "    train_dataset = [dataset[i] for i in train_id]\n",
    "    # Test dataset\n",
    "    eval_dataset = [dataset[i] for i in eval_id]\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_v3',\n",
    "    sub_dir='/batched/'\n",
    ")\n",
    "print('Dataset loaded.')\n",
    "train_dataset, test_dataset = train_eval_split(dataset,\n",
    "    train_id=list(range(0,10)), eval_id=list(range(30,40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from networks.network_parc_v4 import PARC_reducedV4 as PARC\n",
    "# from networks.network_parc import PARC_reduced\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from networks.network_recurrent import objectview\n",
    "from networks.loss import WeightedMSELoss\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model params\n",
    "args = objectview({\n",
    "    'n_fields' : 2,\n",
    "    'n_timesteps' : 1,\n",
    "    'n_hiddenfields' : 128,\n",
    "    'n_meshfields' : 10, # dataset[0].node_attr.size(1),\n",
    "    'n_bcfields' : 1,\n",
    "    'time' : 4.0,\n",
    "    'device' : torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'lr' : 2e-7,\n",
    "    'weight_decay' : 5e-4,\n",
    "    'epoch' : 100,\n",
    "    'train_lambda' : 0.5\n",
    "})\n",
    "\n",
    "# # Model initializing\n",
    "\n",
    "model = PARC(\n",
    "    n_fields=args.n_fields,\n",
    "    n_timesteps=args.n_timesteps,\n",
    "    n_hiddenfields=args.n_hiddenfields,\n",
    "    n_meshfields=args.n_meshfields,\n",
    "    n_bcfields=args.n_bcfields\n",
    ").to(args.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = WeightedMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.4327315027108982; eval loss = 0.4554592967033386\n",
      "Epoch 1: train loss = 0.44231866159529054; eval loss = 0.4629867075624206\n",
      "Epoch 2: train loss = 0.44623501595711057; eval loss = 0.4625476896762848\n",
      "Epoch 3: train loss = 0.44207632210519576; eval loss = 0.4561157518337858\n",
      "Epoch 4: train loss = 0.43671658160301624; eval loss = 0.453199177980423\n",
      "Epoch 5: train loss = 0.43671312915704036; eval loss = 0.4560881555080414\n",
      "Epoch 6: train loss = 0.44168164132776; eval loss = 0.4618586599826813\n",
      "Epoch 7: train loss = 0.4455535392466331; eval loss = 0.46248000860214233\n",
      "Epoch 8: train loss = 0.4428548175583845; eval loss = 0.4578157067298889\n",
      "Epoch 9: train loss = 0.4387847659347942; eval loss = 0.4555663764476776\n",
      "Epoch 10: train loss = 0.43968207106400337; eval loss = 0.45923223922837453\n",
      "Epoch 11: train loss = 0.44384601343602753; eval loss = 0.46231526136398315\n",
      "Epoch 12: train loss = 0.44439467481597167; eval loss = 0.46062415838241577\n",
      "Epoch 13: train loss = 0.4420255315753649; eval loss = 0.4583016677722491\n",
      "Epoch 14: train loss = 0.4405893376663296; eval loss = 0.4580989480018616\n",
      "Epoch 15: train loss = 0.4414734396794557; eval loss = 0.45958200097084045\n",
      "Epoch 16: train loss = 0.44255303443102967; eval loss = 0.45980921387672424\n",
      "Epoch 17: train loss = 0.44179505572129096; eval loss = 0.45814188615081197\n",
      "Epoch 18: train loss = 0.4397607806218745; eval loss = 0.45611628890037537\n",
      "Epoch 19: train loss = 0.438128539788648; eval loss = 0.4545675814151764\n",
      "Epoch 20: train loss = 0.4364072240623538; eval loss = 0.45263344049453735\n",
      "Epoch 21: train loss = 0.43467887092686297; eval loss = 0.4509486258029938\n",
      "Epoch 22: train loss = 0.4326203829712338; eval loss = 0.44822029216484455\n",
      "Epoch 23: train loss = 0.43033263580854086; eval loss = 0.4464093744754791\n",
      "Epoch 24: train loss = 0.42861511484881865; eval loss = 0.44438567757606506\n",
      "Epoch 25: train loss = 0.4268155336005133; eval loss = 0.4427691102027893\n",
      "Epoch 26: train loss = 0.42617237330482693; eval loss = 0.4426405781594962\n",
      "Epoch 27: train loss = 0.4266666143320392; eval loss = 0.44285574555397034\n",
      "Epoch 28: train loss = 0.42643656795629664; eval loss = 0.44184866547584534\n",
      "Epoch 29: train loss = 0.4266124413323352; eval loss = 0.4434293951503386\n",
      "Epoch 30: train loss = 0.4300135588371029; eval loss = 0.4472745059671142\n",
      "Epoch 31: train loss = 0.4341368817058499; eval loss = 0.4509285379130885\n",
      "Epoch 32: train loss = 0.4382284811582705; eval loss = 0.4552288353443146\n",
      "Epoch 33: train loss = 0.44400523131248587; eval loss = 0.46117934584617615\n",
      "Epoch 34: train loss = 0.4511199579173914; eval loss = 0.46899041533470154\n",
      "Epoch 35: train loss = 0.45939973090429725; eval loss = 0.47670045495033264\n",
      "Epoch 36: train loss = 0.47019566442481625; eval loss = 0.48876944184303284\n",
      "Epoch 37: train loss = 0.4822869703204877; eval loss = 0.5012353658676147\n",
      "Epoch 38: train loss = 0.5002114398549438; eval loss = 0.5200507044792175\n",
      "Epoch 39: train loss = 0.5160727434688144; eval loss = 0.5361462812753593\n",
      "Epoch 40: train loss = 0.5398040265157288; eval loss = 0.5569540858268738\n",
      "Epoch 41: train loss = 0.5565022528796326; eval loss = 0.5807985067367554\n",
      "Epoch 42: train loss = 0.5752641682354909; eval loss = 0.5803740620613098\n",
      "Epoch 43: train loss = 0.5866262174252445; eval loss = 0.5982893109321594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_loader\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m()):\n\u001b[1;32m     92\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader))\n\u001b[0;32m---> 93\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39;49mmodel, data\u001b[39m=\u001b[39;49mdata, args\u001b[39m=\u001b[39;49margs)\n\u001b[1;32m     95\u001b[0m train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m train_loader\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m() \u001b[39m# len(train_dataset)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m total_train_loss\u001b[39m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, args)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m# F_dots_hat = F_dots_hat[:,1:,:]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[39m# loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m#         args.train_lambda*criterion(F_dots_hat, F_dots)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(Fs_hat, Fs)\n\u001b[0;32m---> 39\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/geometric/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train function v2\n",
    "def train(model, data, args):\n",
    "    n_time = data.pressure.size(1)\n",
    "    timesteps = args.time / n_time\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([\n",
    "        data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "        )], dim=1).to(args.device)\n",
    "    mesh_features = data.node_attr.to(args.device)\n",
    "    F_bc = data.flowrate_bc.to(args.device)\n",
    "    # weight = data.node_weight.to(args.device)\n",
    "\n",
    "    F_initial = torch.cat([\n",
    "        data.pressure[:,0].unsqueeze(1), \n",
    "        data.flowrate[:,0].unsqueeze(1)\n",
    "    ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "\n",
    "    model.n_timesteps = n_time\n",
    "\n",
    "    Fs, F_dots = model(F_initial, mesh_features, edge_index, F_bc, timesteps)\n",
    "\n",
    "    Fs_hat = torch.cat([\n",
    "        data.pressure.unsqueeze(-1), \n",
    "        data.flowrate.unsqueeze(-1)\n",
    "    ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "    \n",
    "    # F_dots_hat = torch.cat([\n",
    "    #     data.pressure_dot.unsqueeze(-1), \n",
    "    #     data.flowrate_dot.unsqueeze(-1)\n",
    "    # ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "\n",
    "    Fs_hat = Fs_hat[:,1:,:]\n",
    "    # F_dots_hat = F_dots_hat[:,1:,:]\n",
    "    \n",
    "    # loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\n",
    "    #         args.train_lambda*criterion(F_dots_hat, F_dots)\n",
    "    loss = criterion(Fs_hat, Fs)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Eval function\n",
    "def eval(model, data, args):\n",
    "    n_time = data.pressure.size(1)\n",
    "    timesteps = args.time / n_time\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([\n",
    "        data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "        )], dim=1).to(args.device)\n",
    "    mesh_features = data.node_attr.to(args.device)\n",
    "    F_bc = data.flowrate_bc.to(args.device)\n",
    "    # weight = data.node_weight.to(args.device)\n",
    "    F_initial = torch.cat([\n",
    "        data.pressure[:,0].unsqueeze(1), \n",
    "        data.flowrate[:,0].unsqueeze(1)\n",
    "    ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "    model.n_timesteps = n_time\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Fs, F_dots = model(F_initial, mesh_features, edge_index, F_bc, timesteps)\n",
    "\n",
    "        Fs_hat = torch.cat([\n",
    "            data.pressure.unsqueeze(-1), \n",
    "            data.flowrate.unsqueeze(-1)\n",
    "        ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "        \n",
    "        # F_dots_hat = torch.cat([\n",
    "        #     data.pressure_dot.unsqueeze(-1), \n",
    "        #     data.flowrate_dot.unsqueeze(-1)\n",
    "        # ], dim=-1).to(args.device)\n",
    "\n",
    "        Fs_hat = Fs_hat[:,1:,:]\n",
    "        # F_dots_hat = F_dots_hat[:,1:,:]\n",
    "\n",
    "        # loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\n",
    "        #     args.train_lambda*criterion(F_dots_hat, F_dots)\n",
    "        loss = criterion(Fs_hat, Fs)\n",
    "\n",
    "    # return loss\n",
    "    return loss.item()\n",
    "\n",
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "# batch = enumerate(list(range(0,10)))\n",
    "for epoch in range(args.epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = 0\n",
    "    # for data in train_dataset:\n",
    "    for i in range(train_loader.__len__()):\n",
    "        data = next(iter(train_loader))\n",
    "        train_loss += train(model=model, data=data, args=args)\n",
    "\n",
    "    train_loss /= train_loader.__len__() # len(train_dataset)\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    # for data in eval_dataset:\n",
    "    for i in range(test_loader.__len__()):\n",
    "        data = next(iter(test_loader))\n",
    "        eval_loss += eval(model=model, data=data, args=args)\n",
    "    eval_loss /= test_loader.__len__() #len(eval_dataset)\n",
    "    total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    # if (epoch > 25):\n",
    "    #     args.train_lambda = 0.5\n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        torch.save(model.state_dict(), f'models/parc_v4-6_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_train_loss)\n",
    "plt.plot(total_eval_loss)\n",
    "# plt.ylim(0,0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_train_loss)\n",
    "plt.plot(total_eval_loss)\n",
    "# plt.ylim(0,0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct CFD output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initializing\n",
    "model = PARC_reduced(\n",
    "    n_fields=args.n_fields,\n",
    "    n_timesteps=args.n_timesteps,\n",
    "    n_hiddenfields=args.n_hiddenfields,\n",
    "    n_meshfields=args.n_meshfields,\n",
    "    n_bcfields=args.n_bcfields\n",
    ")\n",
    "model = model.to(args.device)\n",
    "model.load_state_dict(torch.load(\n",
    "    'models/parc_v4-5_epoch50.pth',\n",
    "    map_location={'cuda:1': 'cuda:0'}\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction/ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(model, data):\n",
    "    # prepare input tensors\n",
    "    n_time = data.pressure.size(1)\n",
    "    timesteps = args.time / n_time\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([\n",
    "        data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "        )], dim=1).to(args.device)\n",
    "    mesh_features = data.node_attr.to(args.device)\n",
    "    F_bc = data.flowrate_bc.to(args.device)\n",
    "    weight = data.node_weight.to(args.device)\n",
    "    F_initial = torch.cat([\n",
    "        data.pressure[:,0].unsqueeze(1), \n",
    "        data.flowrate[:,0].unsqueeze(1)\n",
    "    ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "    model.n_timesteps = n_time - 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Fs, F_dots = model(F_initial, mesh_features, edge_index, F_bc, timesteps)\n",
    "\n",
    "        Fs_hat = torch.cat([\n",
    "            data.pressure.unsqueeze(-1), \n",
    "            data.flowrate.unsqueeze(-1)\n",
    "        ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "        \n",
    "        # F_dots_hat = torch.cat([\n",
    "        #     data.pressure_dot.unsqueeze(-1), \n",
    "        #     data.flowrate_dot.unsqueeze(-1)\n",
    "        # ], dim=-1).to(args.device)\n",
    "\n",
    "        Fs_hat = Fs_hat[:,1:,:]\n",
    "        # F_dots_hat = F_dots_hat[:,1:,:]\n",
    "\n",
    "        # loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\n",
    "        #     args.train_lambda*criterion(F_dots_hat, F_dots)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # plot\n",
    "    for i_node in [200, 20000]:\n",
    "\n",
    "        i_field = 1\n",
    "        y_pred = Fs.cpu().numpy()[i_node,:,i_field]\n",
    "        y_true = Fs_hat.cpu().numpy()[i_node,:,i_field]\n",
    "        x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "        # print(data.node_attr.numpy()[i_node, 6])\n",
    "        plt.ylim(-1,1)\n",
    "        plt.plot(x, y_pred, c='red', label='GNN Crank-Nicolson')\n",
    "        plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "        # plt.ylim([-1,1])\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('Flowrate', fontsize=20)\n",
    "        plt.xlabel('Time', fontsize=20)\n",
    "        plt.show()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_v3',\n",
    "    sub_dir='/validation/'\n",
    ")\n",
    "print('Dataset loaded.')\n",
    "\n",
    "plot_comparison(model, dataset[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction/ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(model, data):\n",
    "    # prepare input tensors\n",
    "    n_time = data.pressure.size(1)\n",
    "    timesteps = args.time / n_time\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([\n",
    "        data.edge_index, torch.flip(data.edge_index, dims=[0]\n",
    "        )], dim=1).to(args.device)\n",
    "    mesh_features = data.node_attr.to(args.device)\n",
    "    F_bc = data.flowrate_bc.to(args.device)\n",
    "    weight = data.node_weight.to(args.device)\n",
    "    F_initial = torch.cat([\n",
    "        data.pressure[:,0].unsqueeze(1), \n",
    "        data.flowrate[:,0].unsqueeze(1)\n",
    "    ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "    model.n_timesteps = n_time - 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Fs, F_dots = model(F_initial, mesh_features, edge_index, F_bc, timesteps)\n",
    "\n",
    "        Fs_hat = torch.cat([\n",
    "            data.pressure.unsqueeze(-1), \n",
    "            data.flowrate.unsqueeze(-1)\n",
    "        ], dim=-1).to(args.device) # concat pressure and flowrate\n",
    "        \n",
    "        F_dots_hat = torch.cat([\n",
    "            data.pressure_dot.unsqueeze(-1), \n",
    "            data.flowrate_dot.unsqueeze(-1)\n",
    "        ], dim=-1).to(args.device)\n",
    "\n",
    "        Fs_hat = Fs_hat[:,1:,:]\n",
    "        F_dots_hat = F_dots_hat[:,1:,:]\n",
    "\n",
    "        # loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\n",
    "        #     args.train_lambda*criterion(F_dots_hat, F_dots)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # plot\n",
    "    for i_node in [1, 100, 1000, 10000, 50000]:\n",
    "\n",
    "        i_field = 1\n",
    "        y_pred = Fs.cpu().numpy()[i_node,:,i_field]\n",
    "        y_true = Fs_hat.cpu().numpy()[i_node,:,i_field]\n",
    "        x = [i * 4.0 /200 for i in range(y_pred.shape[0])]\n",
    "        print(data.node_attr.numpy()[i_node, 6])\n",
    "        plt.ylim(-1,1)\n",
    "        plt.plot(x, y_pred, c='red', label='PIGNN')\n",
    "        plt.plot(x, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "        # plt.ylim([-1,1])\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('flowrate', fontsize=20)\n",
    "        plt.xlabel('Time', fontsize=20)\n",
    "        plt.show()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_v3',\n",
    "    sub_dir='/validation/'\n",
    ")\n",
    "print('Dataset loaded.')\n",
    "\n",
    "plot_comparison(model, dataset[40])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
