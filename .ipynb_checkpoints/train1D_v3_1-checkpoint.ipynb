{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from data.dataset import OneDDatasetLoader, DatasetLoader\n",
    "from typing import List\n",
    "\n",
    "def train_eval_split(dataset : DatasetLoader, train_id : List, eval_id : List):\n",
    "    # Get batching id\n",
    "    if dataset._sub_dir == '/batched/':\n",
    "        batching_id = dataset.batching_id.numpy()\n",
    "        train_id = list(np.where(np.isin(batching_id, train_id) == True)[0])\n",
    "        eval_id = list(np.where(np.isin(batching_id, eval_id) == True)[0])\n",
    "    # Train dataset\n",
    "    train_dataset = [dataset[i] for i in train_id]\n",
    "    # Test dataset\n",
    "    eval_dataset = [dataset[i] for i in eval_id]\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded.\n",
      "Dataset batching finished.\n",
      "Train/eval spliting finished.\n"
     ]
    }
   ],
   "source": [
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_all',\n",
    "    sub_dir='/normalized/'\n",
    ")\n",
    "print('Dataset loaded.')\n",
    "\n",
    "batched_dataset = dataset.batching(batch_size=None, batch_n_times=10, recursive=True,\n",
    "                                    sub_dir='/batched/')\n",
    "print('Dataset batching finished.')\n",
    "\n",
    "batched_dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_nodeattr_all',\n",
    "    sub_dir='/batched/'\n",
    ")\n",
    "\n",
    "train_dataset, eval_dataset = train_eval_split(\n",
    "    dataset=batched_dataset,\n",
    "    train_id=list(range(0,10)),\n",
    "    eval_id=list(range(20,30))\n",
    ")\n",
    "print('Train/eval spliting finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from networks.network_parc import PARC\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from networks.network_recurrent import objectview\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model params\n",
    "args = objectview({\n",
    "    'n_fields' : 2,\n",
    "    'n_timesteps' : 1,\n",
    "    'n_hiddenfields' : 128,\n",
    "    'n_meshfields' : dataset[0].node_attr.size(1),\n",
    "    'timesteps' : 0.02,\n",
    "    'device' : torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'lr' : 5e-7,\n",
    "    'weight_decay' : 5e-4,\n",
    "    'epoch' : 100,\n",
    "    'train_lambda' : 1.0\n",
    "})\n",
    "\n",
    "# Model initializing\n",
    "model = PARC(\n",
    "    n_fields=args.n_fields,\n",
    "    n_timesteps=args.n_timesteps,\n",
    "    n_hiddenfields=args.n_hiddenfields,\n",
    "    n_meshfields=args.n_meshfields\n",
    ")\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = WeightedMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.6231963068246842; eval loss = 0.6792154014110565\n",
      "Epoch 1: train loss = 0.6112928777933121; eval loss = 0.6654642283916473\n",
      "Epoch 2: train loss = 0.5982132583856583; eval loss = 0.6505459904670715\n",
      "Epoch 3: train loss = 0.5842158854007721; eval loss = 0.6348427414894104\n",
      "Epoch 4: train loss = 0.5696425646543503; eval loss = 0.6186970859766007\n",
      "Epoch 5: train loss = 0.5547743767499924; eval loss = 0.6023745208978653\n",
      "Epoch 6: train loss = 0.5398283064365387; eval loss = 0.5860732734203339\n",
      "Epoch 7: train loss = 0.5249533265829086; eval loss = 0.5699202924966812\n",
      "Epoch 8: train loss = 0.5102532804012299; eval loss = 0.554008275270462\n",
      "Epoch 9: train loss = 0.49579605758190154; eval loss = 0.5383969664573669\n",
      "Epoch 10: train loss = 0.4816319137811661; eval loss = 0.5231304317712784\n",
      "Epoch 11: train loss = 0.46779840886592866; eval loss = 0.5082450151443482\n",
      "Epoch 12: train loss = 0.501396182179451; eval loss = 0.5382350742816925\n",
      "Epoch 13: train loss = 0.49118549525737765; eval loss = 0.5267856001853943\n",
      "Epoch 14: train loss = 0.4805036336183548; eval loss = 0.5149314731359482\n",
      "Epoch 15: train loss = 0.4694951057434082; eval loss = 0.5028521388769149\n",
      "Epoch 16: train loss = 0.4583065778017044; eval loss = 0.4906761169433594\n",
      "Epoch 17: train loss = 0.44704307019710543; eval loss = 0.4784935414791107\n",
      "Epoch 18: train loss = 0.4357810109853745; eval loss = 0.46636873185634614\n",
      "Epoch 19: train loss = 0.4245774120092392; eval loss = 0.4543462425470352\n",
      "Epoch 20: train loss = 0.41347031891345976; eval loss = 0.4424523860216141\n",
      "Epoch 21: train loss = 0.4024840474128723; eval loss = 0.43071465790271757\n",
      "Epoch 22: train loss = 0.391644224524498; eval loss = 0.4191504031419754\n",
      "Epoch 23: train loss = 0.3809689849615097; eval loss = 0.40777944326400756\n",
      "Epoch 24: train loss = 0.37047051787376406; eval loss = 0.39661220610141756\n",
      "Epoch 25: train loss = 0.36015996634960173; eval loss = 0.3856513470411301\n",
      "Epoch 26: train loss = 0.3859447598457336; eval loss = 0.4065619558095932\n",
      "Epoch 27: train loss = 0.3730731099843979; eval loss = 0.392848414182663\n",
      "Epoch 28: train loss = 0.35988725125789645; eval loss = 0.37884359061717987\n",
      "Epoch 29: train loss = 0.34644379913806916; eval loss = 0.3646590292453766\n",
      "Epoch 30: train loss = 0.3328764647245407; eval loss = 0.3504507660865784\n",
      "Epoch 31: train loss = 0.3193323493003845; eval loss = 0.33633664846420286\n",
      "Epoch 32: train loss = 0.3059289649128914; eval loss = 0.3224310040473938\n",
      "Epoch 33: train loss = 0.29276396334171295; eval loss = 0.30882401019334793\n",
      "Epoch 34: train loss = 0.2799306735396385; eval loss = 0.2955935001373291\n",
      "Epoch 35: train loss = 0.26748934090137483; eval loss = 0.2827936917543411\n",
      "Epoch 36: train loss = 0.25548815429210664; eval loss = 0.27046376168727876\n",
      "Epoch 37: train loss = 0.24395920038223268; eval loss = 0.2586283415555954\n",
      "Epoch 38: train loss = 0.23292432725429535; eval loss = 0.24729385375976562\n",
      "Epoch 39: train loss = 0.22239205837249756; eval loss = 0.2364599660038948\n",
      "Epoch 40: train loss = 0.21235771030187606; eval loss = 0.22609640508890153\n",
      "Epoch 41: train loss = 0.20279121696949004; eval loss = 0.2161603480577469\n",
      "Epoch 42: train loss = 0.19367685317993164; eval loss = 0.20664193332195283\n",
      "Epoch 43: train loss = 0.18500229120254516; eval loss = 0.19753099977970123\n",
      "Epoch 44: train loss = 0.17676241993904113; eval loss = 0.18881381154060364\n",
      "Epoch 45: train loss = 0.16894165873527528; eval loss = 0.18048015981912613\n",
      "Epoch 46: train loss = 0.16152944788336754; eval loss = 0.1725257381796837\n",
      "Epoch 47: train loss = 0.1545143321156502; eval loss = 0.16493426263332367\n",
      "Epoch 48: train loss = 0.1478833518922329; eval loss = 0.15769285410642625\n",
      "Epoch 49: train loss = 0.14162682518362998; eval loss = 0.150792396068573\n",
      "Epoch 50: train loss = 0.1357388362288475; eval loss = 0.14424702748656273\n",
      "Epoch 51: train loss = 0.1302237719297409; eval loss = 0.13807337284088134\n",
      "Epoch 52: train loss = 0.12507042065262794; eval loss = 0.13226779028773308\n",
      "Epoch 53: train loss = 0.12024742439389229; eval loss = 0.12680500298738479\n",
      "Epoch 54: train loss = 0.11571241617202759; eval loss = 0.12165147289633751\n",
      "Epoch 55: train loss = 0.11143389940261841; eval loss = 0.11678153425455093\n",
      "Epoch 56: train loss = 0.10737989023327828; eval loss = 0.11217418983578682\n",
      "Epoch 57: train loss = 0.10352277681231499; eval loss = 0.10780933648347854\n",
      "Epoch 58: train loss = 0.09985133558511734; eval loss = 0.10367001071572304\n",
      "Epoch 59: train loss = 0.09635684192180634; eval loss = 0.09974333271384239\n",
      "Epoch 60: train loss = 0.09302562102675438; eval loss = 0.09601194858551025\n",
      "Epoch 61: train loss = 0.08984539359807968; eval loss = 0.09246202781796456\n",
      "Epoch 62: train loss = 0.08680873289704323; eval loss = 0.08906511440873147\n",
      "Epoch 63: train loss = 0.08389452770352364; eval loss = 0.0858003631234169\n",
      "Epoch 64: train loss = 0.08109765276312828; eval loss = 0.08265909552574158\n",
      "Epoch 65: train loss = 0.0784135852009058; eval loss = 0.07964745685458183\n",
      "Epoch 66: train loss = 0.07584669664502144; eval loss = 0.07677156031131745\n",
      "Epoch 67: train loss = 0.0733990453183651; eval loss = 0.07402873449027539\n",
      "Epoch 68: train loss = 0.07106910571455956; eval loss = 0.07140786722302436\n",
      "Epoch 69: train loss = 0.0688476949930191; eval loss = 0.06890512891113758\n",
      "Epoch 70: train loss = 0.06673147641122341; eval loss = 0.06651366800069809\n",
      "Epoch 71: train loss = 0.06471478119492531; eval loss = 0.06422545053064824\n",
      "Epoch 72: train loss = 0.06279447749257087; eval loss = 0.062043309584259985\n",
      "Epoch 73: train loss = 0.06096048392355442; eval loss = 0.05996222235262394\n",
      "Epoch 74: train loss = 0.05922437869012356; eval loss = 0.05797493681311607\n",
      "Epoch 75: train loss = 0.057573411613702774; eval loss = 0.056072556599974635\n",
      "Epoch 76: train loss = 0.056001439690589905; eval loss = 0.05424874313175678\n",
      "Epoch 77: train loss = 0.05450381748378277; eval loss = 0.052497918531298635\n",
      "Epoch 78: train loss = 0.05308014713227749; eval loss = 0.050813551247119906\n",
      "Epoch 79: train loss = 0.05172913186252117; eval loss = 0.04919430650770664\n",
      "Epoch 80: train loss = 0.05044845826923847; eval loss = 0.04764780849218368\n",
      "Epoch 81: train loss = 0.04923981614410877; eval loss = 0.04618871584534645\n",
      "Epoch 82: train loss = 0.0480877336114645; eval loss = 0.04481982514262199\n",
      "Epoch 83: train loss = 0.046994006261229515; eval loss = 0.04353508204221725\n",
      "Epoch 84: train loss = 0.04595450274646282; eval loss = 0.04232707004994154\n",
      "Epoch 85: train loss = 0.04495220072567463; eval loss = 0.04117319229990244\n",
      "Epoch 86: train loss = 0.04399458896368742; eval loss = 0.04006387032568455\n",
      "Epoch 87: train loss = 0.04307040218263865; eval loss = 0.03899230528622866\n",
      "Epoch 88: train loss = 0.042174541391432285; eval loss = 0.03795457407832146\n",
      "Epoch 89: train loss = 0.041305165737867355; eval loss = 0.03694733418524265\n",
      "Epoch 90: train loss = 0.040453164651989934; eval loss = 0.03597017861902714\n",
      "Epoch 91: train loss = 0.03961677867919207; eval loss = 0.03502027820795774\n",
      "Epoch 92: train loss = 0.03879890739917755; eval loss = 0.03409453723579645\n",
      "Epoch 93: train loss = 0.03799957074224949; eval loss = 0.03319074045866728\n",
      "Epoch 94: train loss = 0.037219220958650115; eval loss = 0.032308878377079964\n",
      "Epoch 95: train loss = 0.03645905125886202; eval loss = 0.03144908156245947\n",
      "Epoch 96: train loss = 0.035718221217393875; eval loss = 0.03061079401522875\n",
      "Epoch 97: train loss = 0.0349977221339941; eval loss = 0.029796423949301244\n",
      "Epoch 98: train loss = 0.03429861646145582; eval loss = 0.02901086341589689\n",
      "Epoch 99: train loss = 0.03362516388297081; eval loss = 0.028244542330503462\n"
     ]
    }
   ],
   "source": [
    "# Train function v2\n",
    "def train(model, data, args):\n",
    "    n_time = data.pressure.size(1)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    mesh_features = data.node_attr.to(args.device)\n",
    "    F_initial = torch.cat([data.pressure[:,0].unsqueeze(1), data.velocity[:,0].unsqueeze(1)], dim=-1)\\\n",
    "                .to(args.device) # concat pressure and velocity\n",
    "    model.n_timesteps = n_time - 1\n",
    "\n",
    "    Fs, F_dots = model(F_initial, mesh_features, edge_index)\n",
    "\n",
    "    Fs_hat = torch.cat([data.pressure.unsqueeze(-1), data.velocity.unsqueeze(-1)], dim=-1)\\\n",
    "                .to(args.device) # concat pressure and velocity\n",
    "    \n",
    "    # F_dots_hat = cal_derivative(Fs_hat, dim=1, delta_t=args.timesteps)\n",
    "    F_dots_hat = torch.cat([data.pressure_dot.unsqueeze(-1), data.velocity_dot.unsqueeze(-1)], dim=-1)\\\n",
    "                .to(args.device) # concat pressure and velocity\n",
    "\n",
    "    Fs_hat = Fs_hat[:,1:,:]\n",
    "    F_dots_hat = F_dots_hat[:,1:,:]\n",
    "\n",
    "    # print(Fs_hat, Fs)\n",
    "    loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\n",
    "            args.train_lambda*criterion(F_dots_hat, F_dots)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Eval function\n",
    "def eval(model, data, args):\n",
    "    n_time = data.pressure.size(1)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    mesh_features = data.node_attr.to(args.device)\n",
    "    F_initial = torch.cat([data.pressure[:,0].unsqueeze(1), data.velocity[:,0].unsqueeze(1)], dim=-1)\\\n",
    "                .to(args.device) # concat pressure and velocity\n",
    "    model.n_timesteps = n_time - 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Fs, F_dots = model(F_initial, mesh_features, edge_index)\n",
    "\n",
    "        Fs_hat = torch.cat([data.pressure.unsqueeze(-1), data.velocity.unsqueeze(-1)], dim=-1)\\\n",
    "                    .to(args.device) # concat pressure and velocity\n",
    "        \n",
    "        # F_dots_hat = cal_derivative(Fs_hat, dim=1, delta_t=args.timesteps)\n",
    "        F_dots_hat = torch.cat([data.pressure_dot.unsqueeze(-1), data.velocity_dot.unsqueeze(-1)], dim=-1)\\\n",
    "                .to(args.device)\n",
    "\n",
    "        Fs_hat = Fs_hat[:,1:,:]\n",
    "        F_dots_hat = F_dots_hat[:,1:,:]\n",
    "\n",
    "        loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\n",
    "            args.train_lambda*criterion(F_dots_hat, F_dots)\n",
    "\n",
    "    return loss.item()\n",
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "# batch = enumerate(list(range(0,10)))\n",
    "for epoch in range(args.epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = 0\n",
    "    for data in train_dataset[0:10]:\n",
    "        train_loss += train(model=model, data=data, args=args)\n",
    "    train_loss /= len(train_dataset[0:10])\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    for data in eval_dataset[0:10]:\n",
    "        eval_loss += eval(model=model, data=data, args=args)\n",
    "    eval_loss /= len(eval_dataset[0:10])\n",
    "    total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    if (epoch > 10) and (epoch < 25):\n",
    "        args.train_lambda = 0.75\n",
    "    elif (epoch >=25):\n",
    "        args.train_lambda = 0.5\n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        torch.save(model.state_dict(), f'models/parc_v2_epoch{epoch+1}.pth')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'models/parc_v2_final.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct CFD output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct CFD\n",
    "def print_prediction(model, data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model initializing\n",
    "model = PARC(\n",
    "    n_fields=args.n_fields,\n",
    "    n_timesteps=args.n_timesteps,\n",
    "    n_hiddenfields=args.n_hiddenfields,\n",
    "    n_meshfields=args.n_meshfields\n",
    ")\n",
    "model = model.to(args.device)\n",
    "model.load_state_dict(torch.load('models/parc_v2_final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (59745x1 and 2x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[0;32m---> 50\u001b[0m \u001b[43mplot_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mplot_comparison\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mn_timesteps \u001b[38;5;241m=\u001b[39m n_time \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 13\u001b[0m     Fs, F_dots \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     Fs_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([data\u001b[38;5;241m.\u001b[39mpressure\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\\\n\u001b[1;32m     16\u001b[0m                 \u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# concat pressure and velocity\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# F_dots_hat = cal_derivative(Fs_hat, dim=1, delta_t=args.timesteps)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/tam/python_graph_utilities/networks/network_parc.py:233\u001b[0m, in \u001b[0;36mPARC.forward\u001b[0;34m(self, F_initial, mesh_features, edge_index)\u001b[0m\n\u001b[1;32m    231\u001b[0m F_current \u001b[38;5;241m=\u001b[39m F_initial\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_timesteps):\n\u001b[0;32m--> 233\u001b[0m     F_temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_current\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     F_temp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([feature_map, F_temp], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    235\u001b[0m     F_dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivative_solver(F_temp, edge_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:136\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        x (Tensor): The features.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (59745x1 and 2x128)"
     ]
    }
   ],
   "source": [
    "# Plot prediction/ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(model, data):\n",
    "    n_time = data.pressure.size(1)\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    mesh_features = data.node_attr.to(args.device)\n",
    "    F_initial = torch.cat([data.pressure[:,0].unsqueeze(1)], dim=-1)\\\n",
    "                .to(args.device) # concat pressure and velocity\n",
    "    model.n_timesteps = n_time - 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Fs, F_dots = model(F_initial, mesh_features, edge_index)\n",
    "\n",
    "        Fs_hat = torch.cat([data.pressure.unsqueeze(-1)], dim=-1)\\\n",
    "                    .to(args.device) # concat pressure and velocity\n",
    "        \n",
    "        # F_dots_hat = cal_derivative(Fs_hat, dim=1, delta_t=args.timesteps)\n",
    "        F_dots_hat = torch.cat([data.pressure_dot.unsqueeze(-1)], dim=-1)\\\n",
    "                .to(args.device)\n",
    "\n",
    "        Fs_hat = Fs_hat[:,1:,:]\n",
    "        F_dots_hat = F_dots_hat[:,1:,:]\n",
    "\n",
    "        loss = (1.-args.train_lambda)*criterion(Fs_hat, Fs) + \\\n",
    "            args.train_lambda*criterion(F_dots_hat, F_dots)\n",
    "        \n",
    "        total_loss = loss.item()\n",
    "    print(Fs.size(), Fs_hat.size())\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # plot\n",
    "    i_node = 10\n",
    "    i_field = 0\n",
    "    y_pred = Fs.cpu().numpy()[i_node,:,i_field]\n",
    "    y_true = Fs_hat.cpu().numpy()[i_node,:,i_field]\n",
    "    # x = [i * 4.8 /200 for i in range(201)]\n",
    "    plt.plot(y_pred, c='red', label='PARC')\n",
    "    plt.plot(y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "    # plt.ylim([-50,50])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Pressure', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return total_loss\n",
    "    \n",
    "plot_comparison(model, dataset[40])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
