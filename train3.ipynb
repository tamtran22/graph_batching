{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from dataset import OneDDatasetLoader, OneDDataset\n",
    "from network import EmbeddedNet\n",
    "# from utils import reverse_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset, initialize model and loss criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-7\n",
    "decay = 5e-4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_id = range(0, 31)\n",
    "eval_id = range(31, 41)\n",
    "time_id = [str(i).zfill(3) for i in range(201)]\n",
    "dataset = OneDDatasetLoader(\n",
    "    root='/data1/tam/downloaded_datasets_bcflag1', \n",
    "    # source='/data1/tam/datasets/',\n",
    "    time_id = time_id \n",
    "    # download = False\n",
    ")\n",
    "print(f'{len(dataset.subject_list)} subjects have been loaded.')\n",
    "model = EmbeddedNet(\n",
    "    input_dim_node=3, \n",
    "    input_dim_edge=6, \n",
    "    hidden_dim=128, \n",
    "    output_dim=1,  \n",
    "    num_layers = 5, \n",
    "    emb=False, \n",
    "    add_self_loops=True\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "print(f'Loaded model: {model}')\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.HuberLoss(delta=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, f_prev, f, p_prev, p, edge_index, edge_attr):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # print(x.size(), f_prev.size(), p_prev.size(), edge_attr.size(), edge_index.size())\n",
    "    p_pred, f_pred = model(x.float(), f_prev.float(), p_prev.float(), edge_index, edge_attr.float())\n",
    "    loss = criterion(f_pred, f.float())  + criterion(p_pred, p.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def eval(x, f_prev, f, p_prev, p, edge_index, edge_attr):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p_pred, f_pred = model(x.float(), f_prev.float(), p_prev.float(), edge_index, edge_attr.float())\n",
    "        loss = criterion(f_pred, f.float()) + criterion(p_pred, p.float())\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean_x, std_x):\n",
    "    return (x - mean_x) / (std_x + 1e-15)\n",
    "def reverse_normalize(x_normed, mean_x, std_x):\n",
    "    return x_normed * std_x + mean_x\n",
    "def get_gen_mean_std(f, gen, max_gen=10):\n",
    "    mean = []\n",
    "    std = []\n",
    "    for i in range(0, max_gen + 1):\n",
    "        mean.append(np.mean(f[np.where(gen==i),:]))\n",
    "        std.append(np.std(f[np.where(gen==i),:]))\n",
    "    mean.append(np.mean(f[np.where(gen>i),:]))\n",
    "    std.append(np.std(f[np.where(gen>i),:]))\n",
    "    return mean, std\n",
    "def normalize_gen(x, mean, std, gen, max_gen=10):\n",
    "    arr =  np.array([(x[i] - mean[min(gen[i], max_gen)]) / \\\n",
    "        (std[min(gen[i], max_gen)] + 1e-15) for i in range(len(x))])\n",
    "    return torch.tensor(arr)\n",
    "def reverse_normalize_gen(x, mean, std, gen, max_gen=10):\n",
    "    return np.array([x[i]*(std[min(gen[i], max_gen)] + 1e-15) + \\\n",
    "        mean[min(gen[i], max_gen)] for i in range(len(x))])\n",
    "def normalize_dataset(dataset, device):\n",
    "    x = []\n",
    "    p = []\n",
    "    f= []\n",
    "    edge_attr = []\n",
    "    # for i in range(subject, subject+1):\n",
    "    for i in range(len(dataset.subject_list)):\n",
    "        x.append(dataset[i].x.numpy())\n",
    "        p.append(dataset[i].p.numpy())\n",
    "        # f.append(np.delete(dataset[i].f.numpy(),0,0))\n",
    "        f.append(dataset[i].f.numpy())\n",
    "        edge_attr.append(dataset[i].edge_attr.numpy())\n",
    "    x = np.concatenate(x)\n",
    "    f = np.concatenate(f)\n",
    "    p = np.concatenate(p)\n",
    "    edge_attr = np.concatenate(edge_attr)\n",
    "    gen = edge_attr[:, 2]\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    std_x = np.std(x, axis=0)\n",
    "    mean_edge_attr = np.mean(edge_attr, axis=0)\n",
    "    std_edge_attr = np.std(edge_attr, axis=0)\n",
    "    mean_f = np.mean(f)\n",
    "    std_f = np.std(f)\n",
    "    # mean_f, std_f = get_gen_mean_std(f, np.insert(gen,0,0))\n",
    "    mean_p = np.mean(p)\n",
    "    std_p = np.std(p)\n",
    "    # mean_p, std_p = get_gen_mean_std(p, np.insert(gen,0,0))\n",
    "    data = []\n",
    "    # for i in range(subject, subject+1):\n",
    "    for i in range(len(dataset.subject_list)):\n",
    "        data_i = dataset[i]\n",
    "        gen = dataset[i].edge_attr.numpy()[:,2].astype(np.int32)\n",
    "        data_i.x = normalize(data_i.x, mean_x, std_x)\n",
    "        data_i.edge_attr = normalize(data_i.edge_attr, mean_edge_attr, std_edge_attr)\n",
    "        data_i.p = normalize(data_i.p, mean_p, std_p)\n",
    "        data_i.f = normalize(data_i.f[1:,:], mean_f, std_f)\n",
    "        # data_i.p = normalize_gen(data_i.p.numpy(), mean_p, std_p, np.insert(gen,0,0))\n",
    "        # data_i.f = normalize_gen(data_i.f.numpy(), mean_f, std_f, np.insert(gen,0,0))\n",
    "        data.append(data_i.to(device))\n",
    "    return data, mean_p, std_p, mean_f, std_f\n",
    "subject = 35\n",
    "# data[0].x\n",
    "data, mean_p, std_p, mean_f, std_f = normalize_dataset(dataset, device)\n",
    "\n",
    "# print(dataset.subject_list)\n",
    "# train_id = range(0, 150)\n",
    "# test_id = range(200, 240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "print('Start training.')\n",
    "train_loss_all = []\n",
    "eval_loss_all = []\n",
    "for epoch in range(100):\n",
    "    train_loss = 0.0\n",
    "    for i in train_id:\n",
    "        x = data[i].x #.to(device)\n",
    "        edge_index = data[i].edge_index #.to(device)\n",
    "        edge_attr = data[i].edge_attr #.to(device)\n",
    "        for time in range(1, len(time_id)):\n",
    "            p_prev = data[i].p[:,time - 1].unsqueeze(1) #.to(device)\n",
    "            p = data[i].p[:,time].unsqueeze(1) #.to(device)\n",
    "            f_prev = data[i].f[:,time - 1].unsqueeze(1) #.to(device)\n",
    "            f = data[i].f[:,time].unsqueeze(1) #.to(device)\n",
    "            # print(f_prev.size(), edge_attr.size())\n",
    "            train_loss += train(x, f_prev, f, p_prev, p, edge_index, edge_attr)\n",
    "    train_loss /= len(train_id)\n",
    "    train_loss_all.append(train_loss)\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    for i in eval_id:\n",
    "        x_eval = data[i].x\n",
    "        edge_index_eval = data[i].edge_index\n",
    "        edge_attr_eval = data[i].edge_attr\n",
    "        for time in range(1, len(time_id)):\n",
    "            p_prev_eval = data[i].p[:,time - 1].unsqueeze(1)\n",
    "            p_eval = data[i].p[:,time].unsqueeze(1)\n",
    "            f_prev_eval = data[i].f[:,time - 1].unsqueeze(1)\n",
    "            f_eval = data[i].f[:,time].unsqueeze(1)\n",
    "            eval_loss += eval(x_eval, f_prev_eval, f_eval, p_prev_eval, p_eval,\\\n",
    "                        edge_index_eval, edge_attr_eval)\n",
    "    eval_loss /= len(eval_id)\n",
    "    eval_loss_all.append(eval_loss)\n",
    "    \n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch}: train loss={train_loss}; eval loss={eval_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/mgn_pq_v1.pth')\n",
    "plt.plot(train_loss_all, label='train_loss')\n",
    "plt.plot(eval_loss_all, label='eval_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "f = open('./predict_3/loss_mgn_pq_v1','w+')\n",
    "for i in range(len(train_loss_all)):\n",
    "    f.write(str(train_loss_all[i]))\n",
    "    f.write('    ')\n",
    "    f.write(str(eval_loss_all[i]))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write prediction into 1D data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "out_dir = './predict_3/'\n",
    "model = EmbeddedNet(\n",
    "    input_dim_node=3, \n",
    "    input_dim_edge=6, \n",
    "    hidden_dim=128, \n",
    "    output_dim=1,  \n",
    "    num_layers = 5, \n",
    "    emb=False, \n",
    "    add_self_loops=True\n",
    ")\n",
    "device = torch.device(\"cpu\")\n",
    "model.load_state_dict(torch.load('models/mgn_pq_v1.pth'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def print_1D(x, edge_index, f, p, time_name, path):\n",
    "    fo = open(path+'/plt_nd_000'+time_name+'.dat','w+')\n",
    "    fo.write(f'VARIABLES=\"x\" \"y\" \"z\" \"p\" \"f\"\\n')\n",
    "    fo.write(f'ZONE T= \"plt_nd_000{time_name}.dat\"\\n')\n",
    "    fo.write(f'N= {np.shape(x)[0]}, E= {np.shape(edge_index)[1]},F=FEPoint,ET=LINESEG\\n')\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        fo.write(f'{x[i][0]}\\t{x[i][1]}\\t{-1*x[i][2]}\\t{p[i]}\\t{f[i]}\\n')\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        fo.write(f'{edge_index[0][i] + 1}\\t{edge_index[1][i] + 1}\\n')\n",
    "    fo.close()\n",
    "import os\n",
    "os.system('rm -rf {out_dir}*')\n",
    "\n",
    "for i in range(210, 211):\n",
    "    os.system('mkdir '+out_dir+dataset.subject_list[i])\n",
    "    x = data[i].x.to(device)\n",
    "    gen = dataset[i].edge_attr.numpy()[:,2].astype(np.int32)\n",
    "    edge_index = data[i].edge_index.to(device)\n",
    "    edge_attr = data[i].edge_attr.to(device)\n",
    "    out_p = []\n",
    "    out_f = []\n",
    "    out_p.append(data[i].p[:,0].unsqueeze(1).cpu().numpy())\n",
    "    out_f.append(data[i].f[:,0].unsqueeze(1).cpu().numpy())\n",
    "    for time in range(1, len(time_id)):\n",
    "        f_prev = data[i].f[:,time - 1].unsqueeze(1).to(device)\n",
    "        p_prev = data[i].p[:,time - 1].unsqueeze(1).to(device)\n",
    "        p, f = model(x.float(), f_prev.float(), p_prev.float(),edge_index, edge_attr.float())\n",
    "        # p, f = model(x, f_prev, p_prev, edge_attr, edge_index)\n",
    "        out_p.append(p.detach().cpu().numpy())\n",
    "        out_f.append(f.detach().cpu().numpy())\n",
    "\n",
    "    out_p = np.concatenate(out_p, axis=-1)\n",
    "    out_p = reverse_normalize_gen(out_p, mean_p, std_p, np.insert(gen,0,0))\n",
    "\n",
    "    out_f = np.concatenate(out_f, axis=-1)\n",
    "    out_f = reverse_normalize_gen(out_f, mean_f, std_f, np.insert(gen,0,0))\n",
    "\n",
    "    for j in range(len(time_id)):\n",
    "        p = out_p[:,j]\n",
    "        # f = np.insert(out_f[:,j],0,out_f[0,j])\n",
    "        print_1D(\n",
    "            x = dataset[i].x.numpy()*1e-3, \n",
    "            edge_index=dataset[i].edge_index.numpy(),\n",
    "            f = f, \n",
    "            p = p, \n",
    "            time_name = time_id[j], \n",
    "            path=out_dir+dataset.subject_list[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize prediction vs groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time his\n",
    "i = 35\n",
    "print(dataset.subject_list[i])\n",
    "path = out_dir+dataset.subject_list[i]+'/'\n",
    "# path_ref = './predict_2/'+dataset.subject_list[i]+'/'\n",
    "node_id = 0\n",
    "time_list = [i * 4.8 / 200 for i in range(201)]\n",
    "def read_value(path, node_id):\n",
    "    f = open(path, 'r')\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    for i in range(node_id + 1):\n",
    "        line = f.readline()\n",
    "    line = line[:-1].split('\\t')\n",
    "    f.close()\n",
    "    return float(line[-1])\n",
    "value = []\n",
    "value_ref = []\n",
    "for time in time_id:\n",
    "    value.append(read_value(path+'plt_nd_000'+time+'.dat',node_id))\n",
    "    # value_ref.append(read_value(path_ref+'plt_nd_000'+time+'.dat',node_id))\n",
    "y_pred = np.array(value)\n",
    "# y_ref = np.array(value_ref)\n",
    "y_true = dataset[i].f[node_id].numpy()\n",
    "# plt.plot(time_list, y_ref, c='black', label='GCN')\n",
    "plt.plot(time_list, y_pred, c='red', label='ResGCN')\n",
    "plt.plot(time_list, y_true, c='blue', linestyle='dashdot', label='ground_truth')\n",
    "# plt.ylim([-50,50])\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Pressure', fontsize=20)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db637ecd267e8785137d274d569a1de945f5091a2538863d3f2fda8d3b4d9cce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
